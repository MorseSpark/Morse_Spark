{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minimal-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "#findspark.init() \n",
    "SPARK_HOME='/opt/cloudera/parcels/CDH/lib/spark'\n",
    "# SPARK_HOME='/home/qiany/.conda/envs/py37'\n",
    "# os.environ['SPARK_HOME'] = '/home/qiany/.conda/envs/py37'\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "celtic-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "#from hdfs import InsecureClient\n",
    "import numpy as np\n",
    "#from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import coalesce,broadcast,collect_list, collect_set, udf, array_remove, log, lit, first, col, array, sort_array,split, explode, desc, asc, row_number,isnan, when, count\n",
    "from pyspark.sql.types import *\n",
    "import rtree\n",
    "from pyspark.sql import Window\n",
    "#import igraph\n",
    "#from igraph import Graph\n",
    "import geofeather\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guilty-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Here is a programe to extract V1-Paths, please input the absolute or relative path to your TIN file: SigSpatial_data/Canyon_Lake_Gorge_TX.off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************\n",
      "tin_directory:  SigSpatial_data\n",
      "tin_basename:  Canyon_Lake_Gorge_TX.off\n",
      "tin_filename:  Canyon_Lake_Gorge_TX\n",
      "tin_extension:  .off\n",
      "\n",
      "********************\n",
      "This is a TIN file in \".off\" format\n"
     ]
    }
   ],
   "source": [
    "tin_file = input(\"Here is a programe to compute the Forman gradient, please input the absolute or relative path to your TIN file:\")\n",
    "print(\"\\n********************\")\n",
    "\n",
    "# get the directory to the TIN file\n",
    "tin_directory = os.path.dirname(tin_file)\n",
    "print(\"tin_directory: \", tin_directory)\n",
    "\n",
    "directory_type = input(\"Is the data stored in hdfs(0) or SigSpatial(1):\") or \"1\"\n",
    "if directory_type == '0':\n",
    "    directory = 'hdfs_data'\n",
    "else:\n",
    "    directory = 'SigSpatial_data'\n",
    "    \n",
    "# get the basename to the TIN file\n",
    "tin_basename = os.path.basename(tin_file) # input_vertices_2.off\n",
    "print(\"tin_basename: \", tin_basename)\n",
    "\n",
    "# get the filename of the TIN file\n",
    "tin_filename = os.path.splitext(tin_basename)[0] # input_vertices_2\n",
    "print(\"tin_filename: \", tin_filename)\n",
    "\n",
    "# get the type of TIN file: off, tri, etc\n",
    "tin_extension = os.path.splitext(tin_basename)[1] # .off\n",
    "print(\"tin_extension: \", tin_extension)\n",
    "print(\"\\n********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solid-underwear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "spark.sql.shuffle.partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# filtra = input(\"Do you have filtration data?\")\n",
    "# filtra = 'yes'\n",
    "\n",
    "# if filtra.lower() == 'no':\n",
    "#     Basic_Data = input(\"Do you have basic pts and tri data?\")\n",
    "    \n",
    "# Num_executor = input(\"spark.executor.instances:\")\n",
    "# Num_core_per_executor = input(\"spark.executor.cores:\")\n",
    "# Memory_executor = input(\"spark.executor.memory? Please end with 'g':\")\n",
    "# MemoryOverhead_executor = input(\"spark.executor.memoryOverhead? Please end with 'g':\")\n",
    "\n",
    "# Num_core_per_driver = Num_core_per_executor\n",
    "# Memory_driver = Memory_executor\n",
    "# MemoryOverhead_driver = MemoryOverhead_executor\n",
    "Num_core_per_driver = '5'\n",
    "Memory_driver = '20g'\n",
    "MemoryOverhead_driver = '4g'\n",
    "\n",
    "\n",
    "Num_shuffle_partitions = input(\"spark.sql.shuffle.partitions:\")\n",
    "# StorageFraction = input(\"spark.memory.storageFraction:\")\n",
    "# Num_subgraphs = int(input(\"How many subgraphs:\"))\n",
    "\n",
    "Num_executor = '64'\n",
    "Num_core_per_executor = '5'\n",
    "Memory_executor = '64g'\n",
    "MemoryOverhead_executor = '8g'\n",
    "# Num_shuffle_partitions = '20'\n",
    "# StorageFraction = '0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "short-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType, MapType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD, PointRDD, CircleRDD, PolygonRDD, LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "given-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "#os.environ['PYSPARK_PYTHON'] = \"/home/qiany/.conda/envs/py37/bin/python\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chief-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_app_name: Save_PtsFiltra_DataFrame_Canyon_Lake_Gorge_TX_07112024_2147\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "spark.executor.cores: # Number of concurrent tasks an executor can run, euqals to the number of cores to use on each executor\n",
    "spark.executor.instances: # Number of executors for the spark application\n",
    "spark.executor.memory: # Amount of memory to use for each executor that runs the task\n",
    "spark.executor.memoryOverhead:\n",
    "spark.driver.cores: # Number of cores to use for the driver process; the default number is 1\n",
    "spark.driver.memory: # Amount of memory to use for the driver\n",
    "spark.driver.maxResultSize: to define the maximum limit of the total size of the serialized result that a driver can store for each Spark collect action\n",
    "spark.default.parallelism: # Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user. It can be set as spark.executor.instances * spark.executor.cores * 2\n",
    "spark.sql.shuffle.partitions: determine how many partitions are used when data is shuffled between nodes, e.g., joins or aggregations. usually 1~5 times of executor.instances * executor.cores\n",
    "spark.memory.storageFraction: determines the fraction of the heap space that is allocated to caching RDDs and DataFrames in memory.\n",
    "spark.kryoserializer.buffer.max: determine the maximum of data that can be serialized at once; this must be larger than any object we attempt to serialize\n",
    "spark.rpc.message.maxSize: # Maximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map output size information sent between executors and the driver. To communicate between the nodes, Spark uses a protocol called RPC (Remote Procedure Call), which sends messages back and forth. The spark.rpc.message.maxSize parameter limits how big these messages can be. \n",
    "spark.sql.broadcastTimeout: Spark will wait for this amount of time before giving up on broadcasting a table. Broadcasting can take a long time if the table is large or if there is a shuffle operation before it.\n",
    "spark.sql.autoBroadcastJoinThreshold: Spark will broadcast a table to all worker nodes when performing a join if its size is less than this value; -1 means disabling broadcasting\n",
    "'''\n",
    "# .config('spark.memory.storageFraction', StorageFraction) \\\n",
    "\n",
    "date = time.strftime(\"%m,%d,%Y\")\n",
    "date_name = date.split(',')[0] + date.split(',')[1] + date.split(',')[2]\n",
    "\n",
    "hour = time.strftime(\"%H,%M\")\n",
    "hour_name = hour.split(',')[0] + hour.split(',')[1]\n",
    "\n",
    "spark_app_name = \"Save_PtsFiltra_DataFrame_\" + tin_filename + '_' + date_name + '_' + hour_name\n",
    "print(\"spark_app_name:\", spark_app_name)\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(spark_app_name) \\\n",
    ".master('yarn') \\\n",
    ".config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    ".config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    ".config('spark.jars','sedona-core-2.4_2.11-1.0.0-incubating.jar,sedona-sql-2.4_2.11-1.0.0-incubating.jar,sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,sedona-viz-2.4_2.11-1.0.0-incubating.jar,geotools-wrapper-geotools-24.0.jar,graphframes-0.8.0-spark2.4-s_2.11.jar') \\\n",
    ".config('spark.executor.cores', Num_core_per_executor) \\\n",
    ".config('spark.executor.instances', Num_executor) \\\n",
    ".config('spark.executor.memory', Memory_executor) \\\n",
    ".config('spark.executor.memoryOverhead', MemoryOverhead_executor) \\\n",
    ".config('spark.driver.cores', Num_core_per_driver) \\\n",
    ".config('spark.driver.memory', Memory_driver) \\\n",
    ".config('spark.driver.memoryOverhead', MemoryOverhead_driver) \\\n",
    ".config('spark.driver.maxResultSize', '0') \\\n",
    ".config('spark.dynamicAllocation.enabled', 'false') \\\n",
    ".config('spark.network.timeout', '10000001s') \\\n",
    ".config('spark.executor.heartbeatInterval', '10000000s') \\\n",
    ".config('spark.sql.shuffle.partitions', Num_shuffle_partitions) \\\n",
    ".config(\"spark.default.parallelism\", '20') \\\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"1024mb\") \\\n",
    ".config('spark.rpc.message.maxSize', '256') \\\n",
    ".config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    ".config('spark.yarn.dist.archives', '/local/data/yuehui/py37.tar.gz#environment') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-palestine",
   "metadata": {},
   "source": [
    "### read the TIN and save its vertices and triangles as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "# url_in = '/home/qiany/yuehui/pyspark/data/B4.off'\n",
    "# url_pt_origin_csv = '/home/qiany/yuehui/pyspark/data/B4_pts_origin.csv'\n",
    "# url_tri_origin_csv = '/home/qiany/yuehui/pyspark/data/B4_tri_origin.csv'\n",
    "# rank points with filtration.txt file\n",
    "\n",
    "url_in = tin_file\n",
    "url_pt_origin_csv = tin_directory + '/' + tin_filename + '_pts_origin.csv'\n",
    "url_tri_origin_csv = tin_directory + '/' + tin_filename + '_filtra_tri_origin.csv'\n",
    "\n",
    "if tin_extension == \".off\":\n",
    "    with open(url_in) as infile:\n",
    "        trash = infile.readline()\n",
    "        line = (infile.readline()).split()\n",
    "        vertices_num = int(line[0])\n",
    "        triangles_num = int(line[1])\n",
    "        print(\"vnum: {}, tnum:{}\".format(vertices_num, triangles_num))\n",
    "        with open(url_pt_origin_csv, 'w', newline='') as ofs_pts:\n",
    "            writer = csv.writer(ofs_pts)\n",
    "            for l in range(vertices_num):\n",
    "                line = infile.readline().split()\n",
    "                v = [float(line[0]),float(line[1]),float(line[2]),l] # x,y,ele,self_index\n",
    "                writer.writerow(v)\n",
    "            ofs_pts.close()\n",
    "            \n",
    "        with open(url_tri_origin_csv, 'w', newline='') as ofs_tri:\n",
    "            writer = csv.writer(ofs_tri)\n",
    "            tri_order = 0\n",
    "            for l in range(triangles_num):\n",
    "                line = infile.readline().split()\n",
    "                tri = [int(line[1]),int(line[2]),int(line[3]), tri_order]        \n",
    "                writer.writerow(tri)\n",
    "                tri_order += 1\n",
    "            ofs_tri.close()    \n",
    "        infile.close()\n",
    "\n",
    "if tin_extension == \".tri\":\n",
    "    with open(url_in) as infile:\n",
    "        # trash = infile.readline()\n",
    "        line = (infile.readline()).split()\n",
    "        vertices_num = int(line[0])\n",
    "        # triangles_num = int(line[1])\n",
    "        print(\"vnum: {}\".format(vertices_num))\n",
    "        with open(url_pt_origin_csv, 'w', newline='') as ofs_pts:\n",
    "            writer = csv.writer(ofs_pts)\n",
    "            for l in range(vertices_num):\n",
    "                line = infile.readline().split()\n",
    "                v = [float(line[0]),float(line[1]),float(line[2]),l] # x,y,ele,self_index, ele_order\n",
    "                writer.writerow(v)\n",
    "            ofs_pts.close()\n",
    "            \n",
    "        line = (infile.readline()).split()\n",
    "        triangles_num = int(line[0])\n",
    "        print(\"tnum: {}\".format(triangles_num))\n",
    "        with open(url_tri_origin_csv, 'w', newline='') as ofs_tri:\n",
    "            writer = csv.writer(ofs_tri)\n",
    "            tri_order = 0\n",
    "            for l in range(triangles_num):\n",
    "                line = infile.readline().split()\n",
    "                tri = [int(line[0]),int(line[1]),int(line[2]), tri_order]        \n",
    "                writer.writerow(tri)\n",
    "                tri_order += 1\n",
    "            ofs_tri.close()    \n",
    "        infile.close()\n",
    "        \n",
    "t1 = time.time()\n",
    "print(\"time cost:\", t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-robertson",
   "metadata": {},
   "source": [
    "### sort the vertices of a mesh and get its filtration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "changing-klein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- ele: float (nullable = true)\n",
      " |-- self_index: long (nullable = true)\n",
      " |-- self_order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StructType is a collection of StructFieldâ€™s which is used to define the column name, data type, and a flag for nullable or not.\n",
    "# StructType: https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/\n",
    "schema_ver_origin = StructType([ \\\n",
    "    StructField(\"x\",FloatType(),True), \\\n",
    "    StructField(\"y\",FloatType(),True), \\\n",
    "    StructField(\"ele\",FloatType(),True), \\\n",
    "    StructField(\"self_index\",LongType(),True) \\\n",
    "  ])\n",
    "\n",
    "# hdfs_tin_pts_origin = \"hdfs_data\" + \"/\" + tin_filename + '_pts_origin.csv'\n",
    "hdfs_tin_pts_origin = \"SigSpatial_data\" + \"/\" + tin_filename + '_pts_origin.csv'\n",
    "\n",
    "df_ver_origin = spark.read.format(\"csv\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_ver_origin)\\\n",
    "      .load(hdfs_tin_pts_origin)\n",
    "\n",
    "# print(\"Number of partitions:\", df_ver_origin.rdd.getNumPartitions())\n",
    "# df_ver_origin.printSchema()\n",
    "\n",
    "# define a window for the ordering\n",
    "# row_number() function along with partitionBy() of other column populates the row number by group\n",
    "# Since we want to order the whole DataFrame, so we don't need the partitionBy() function\n",
    "w = Window().orderBy(col('ele').asc())\n",
    "df_ver_order = df_ver_origin.withColumn(\"self_order\", F.row_number().over(w) -1) # let the row number start from 0\n",
    "# df_ver_order = df_ver_origin.withColumn(\"self_order\", col(\"self_order\").cast(LongType())) # change the data type of self_order to LongType\n",
    "\n",
    "df_ver_order.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-romania",
   "metadata": {},
   "source": [
    "### write the DataFrame to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "proper-valve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for saving df_ver_format to a csv file: 0.8290736675262451\n"
     ]
    }
   ],
   "source": [
    "# save df_ver_order\n",
    "t0 = time.time()\n",
    "\n",
    "file_df_ver_order_hdfs = 'Sigspatial_data' + '/' + tin_filename + '_filtra_pts_origin.csv'\n",
    "\n",
    "# df_ver_order.write.mode('overwrite').options(header='False', delimiter=',').csv(file_df_ver_order_hdfs)\n",
    "df_ver_order.write.csv(file_df_ver_order_hdfs)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost for saving df_ver_format to a csv file:\", t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solar-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_tri_order() is a function used to read triangles from a csv file\n",
    "def read_tri_order(filtra, directory, tin_filename):\n",
    "    '''\n",
    "    this function has two input parameters.\n",
    "    filtra: 'yes' or 'no', yes means that the input csv file is ordered by default\n",
    "    directory: a string denoting the directory to a TIN file\n",
    "    tin_filename: a string denoting the file name of a tin without extension, e.g., 827_monviso\n",
    "    '''\n",
    "    if filtra.lower() == 'yes':\n",
    "        hdfs_tin_tri_origin = directory + \"/\" + tin_filename + '_filtra_tri_origin.csv'\n",
    "    else: # filtra.lower() == 'no'\n",
    "        hdfs_tin_tri_origin = directory + \"/\" + tin_filename + '_tri_origin.csv'\n",
    "        \n",
    "    schema_tri_origin = StructType([ \\\n",
    "        StructField(\"v1\",IntegerType(),True), \\\n",
    "        StructField(\"v2\",IntegerType(),True), \\\n",
    "        StructField(\"v3\",IntegerType(),True), \\\n",
    "        StructField(\"tri_order\",IntegerType(),True) \\\n",
    "      ])\n",
    "\n",
    "    df_tri_origin = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", False) \\\n",
    "          .schema(schema_tri_origin)\\\n",
    "          .load(hdfs_tin_tri_origin)\n",
    "    print(\"Number of partitions for df_tri_origin:\", df_tri_origin.rdd.getNumPartitions())\n",
    "        \n",
    "    return df_tri_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entitled-october",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_tri_origin: 27\n",
      "root\n",
      " |-- v1: integer (nullable = true)\n",
      " |-- v2: integer (nullable = true)\n",
      " |-- v3: integer (nullable = true)\n",
      " |-- tri_order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read triangles\n",
    "df_tri_origin = read_tri_order(filtra, directory, tin_filename)\n",
    "df_tri_origin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "annoying-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_ver() replaces the original index of each vertex with filtra value in df_tri_origin\n",
    "def replace_ver(df_ver_order, df_tri_origin):\n",
    "    '''\n",
    "    df_ver_order: a DataFrame storing sorted vertices with filtra values\n",
    "    df_tri_origin: a DataFrame storing triangles from a TIN\n",
    "    '''\n",
    "    df_tri_order_v1 = df_tri_origin.join(df_ver_order, df_tri_origin.v1 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v1 = df_tri_order_v1.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"),col(\"self_order\").alias(\"r1\"), col(\"ele\").alias(\"r1_ele\"))\n",
    "\n",
    "    df_tri_order_v2 = df_tri_order_v1.join(df_ver_order, df_tri_order_v1.v2 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v2 = df_tri_order_v2.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"), col(\"r1\"), col(\"self_order\").alias(\"r2\"), col(\"r1_ele\"), col(\"ele\").alias(\"r2_ele\"))\n",
    "\n",
    "    df_tri_order_v3 = df_tri_order_v2.join(df_ver_order, df_tri_order_v2.v3 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v3 = df_tri_order_v3.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"), col(\"r1\"), col(\"r2\"), col(\"self_order\").alias(\"r3\"), col(\"r1_ele\"), col(\"r2_ele\"), col(\"ele\").alias(\"r3_ele\"))\n",
    "\n",
    "    df_tri_order = df_tri_order_v3.select(col(\"tri_order\"), col(\"r1\"), col(\"r2\"), col(\"r3\"), col(\"r1_ele\"), col(\"r2_ele\"), col(\"r3_ele\"))\n",
    "\n",
    "    return df_tri_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "liquid-julian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tri_order: integer (nullable = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- r2_ele: float (nullable = true)\n",
      " |-- r3_ele: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replaces original vertex index with filtra values\n",
    "df_tri_order = replace_ver(df_ver_order, df_tri_origin)\n",
    "df_tri_order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_tri_order\n",
    "t0 = time.time()\n",
    "\n",
    "file_df_tri_order_hdfs = 'Sigspatial_data' + '/' + tin_filename + '_filtra_tri_origin.csv'\n",
    "\n",
    "# df_ver_order.write.mode('overwrite').options(header='False', delimiter=',').csv(file_df_ver_order_hdfs)\n",
    "df_tri_order.write.csv(file_df_tri_order_hdfs)\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Time cost for saving df_ver_format to a csv file:\", t1-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
