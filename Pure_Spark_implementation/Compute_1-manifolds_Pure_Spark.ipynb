{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decreased-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "#findspark.init() \n",
    "SPARK_HOME='/opt/cloudera/parcels/CDH/lib/spark'\n",
    "# SPARK_HOME='/home/qiany/.conda/envs/py37'\n",
    "# os.environ['SPARK_HOME'] = '/home/qiany/.conda/envs/py37'\n",
    "findspark.init(SPARK_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "returning-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "#from hdfs import InsecureClient\n",
    "import numpy as np\n",
    "#from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import create_map\n",
    "from pyspark.sql.functions import array_union,flatten,array_sort,coalesce,broadcast,collect_list, collect_set, udf, array_remove, log, lit, first, col, array, sort_array,split, explode, desc, asc, row_number,isnan, when, count\n",
    "from pyspark.sql.types import *\n",
    "import rtree\n",
    "from pyspark.sql import Window\n",
    "#import igraph\n",
    "#from igraph import Graph\n",
    "import geofeather\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "twenty-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Here is a programe to compute the Forman gradient, please input the absolute or relative path to your TIN file: SigSpatial_data/Canyon_Lake_Gorge_TX.off\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************\n",
      "tin_directory:  SigSpatial_data\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Is the data stored in hdfs(0) or SigSpatial(1): 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tin_basename:  Canyon_Lake_Gorge_TX.off\n",
      "tin_filename:  Canyon_Lake_Gorge_TX\n",
      "tin_extension:  .off\n",
      "\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "tin_file = input(\"Here is a programe to compute the Forman gradient, please input the absolute or relative path to your TIN file:\")\n",
    "print(\"\\n********************\")\n",
    "\n",
    "# get the directory to the TIN file\n",
    "tin_directory = os.path.dirname(tin_file)\n",
    "print(\"tin_directory: \", tin_directory)\n",
    "\n",
    "directory_type = input(\"Is the data stored in hdfs(0) or SigSpatial(1):\") or \"1\"\n",
    "if directory_type == '0':\n",
    "    directory = 'hdfs_data'\n",
    "else:\n",
    "    directory = 'SigSpatial_data'\n",
    "    \n",
    "# get the basename to the TIN file\n",
    "tin_basename = os.path.basename(tin_file) # input_vertices_2.off\n",
    "print(\"tin_basename: \", tin_basename)\n",
    "\n",
    "# get the filename of the TIN file\n",
    "tin_filename = os.path.splitext(tin_basename)[0] # input_vertices_2\n",
    "print(\"tin_filename: \", tin_filename)\n",
    "\n",
    "# get the type of TIN file: off, tri, etc\n",
    "tin_extension = os.path.splitext(tin_basename)[1] # .off\n",
    "print(\"tin_extension: \", tin_extension)\n",
    "print(\"\\n********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "democratic-affect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you have filtration data? yes\n",
      "spark.executor.instances: 64\n",
      "spark.executor.cores: 5\n",
      "spark.executor.memory? Please end with 'g': 64g\n",
      "spark.executor.memoryOverhead? Please end with 'g': 8g\n",
      "spark.sql.shuffle.partitions: 200\n"
     ]
    }
   ],
   "source": [
    "# filtra is the order of each vertex, the order is obtained by ranking elevation values of vertices\n",
    "filtra = input(\"Do you have filtration data?\") or \"yes\"\n",
    "# filtra = 'yes'\n",
    "\n",
    "if filtra.lower() == 'no':    \n",
    "    Basic_Data = input(\"Do you have basic pts and tri data?\")\n",
    "    \n",
    "Num_executor = input(\"spark.executor.instances:\") or \"64\"\n",
    "Num_core_per_executor = input(\"spark.executor.cores:\") or \"5\"\n",
    "Memory_executor = input(\"spark.executor.memory? Please end with 'g':\") or \"64g\"\n",
    "MemoryOverhead_executor = input(\"spark.executor.memoryOverhead? Please end with 'g':\") or \"8g\"\n",
    "\n",
    "# Num_core_per_driver = Num_core_per_executor\n",
    "# Memory_driver = Memory_executor\n",
    "# MemoryOverhead_driver = MemoryOverhead_executor\n",
    "\n",
    "Num_core_per_driver = '5'\n",
    "Memory_driver = '12g'\n",
    "MemoryOverhead_driver = '4g'\n",
    "\n",
    "Num_shuffle_partitions = input(\"spark.sql.shuffle.partitions:\") or \"200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pregnant-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, FloatType, ArrayType, MapType\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.core.SpatialRDD import SpatialRDD, PointRDD, CircleRDD, PolygonRDD, LineStringRDD\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.spatialOperator import KNNQuery\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "from sedona.core.spatialOperator import JoinQueryRaw\n",
    "from sedona.core.spatialOperator import RangeQuery\n",
    "from sedona.core.spatialOperator import RangeQueryRaw\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.formatMapper import WkbReader\n",
    "from sedona.core.formatMapper import WktReader\n",
    "from sedona.core.formatMapper import GeoJsonReader\n",
    "from sedona.sql.types import GeometryType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.SpatialRDD import RectangleRDD\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ready-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "#os.environ['PYSPARK_PYTHON'] = \"/home/qiany/.conda/envs/py37/bin/python\"\n",
    "os.environ['YARN_CONF_DIR'] = \"/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sublime-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_app_name: Compute_2-manifolds_Canyon_Lake_Gorge_TX_07112024_2030\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "spark.executor.cores: # Number of concurrent tasks an executor can run, euqals to the number of cores to use on each executor\n",
    "spark.executor.instances: # Number of executors for the spark application\n",
    "spark.executor.memory: # Amount of memory to use for each executor that runs the task\n",
    "spark.executor.memoryOverhead:\n",
    "spark.driver.cores: # Number of cores to use for the driver process; the default number is 1\n",
    "spark.driver.memory: # Amount of memory to use for the driver\n",
    "spark.driver.maxResultSize: to define the maximum limit of the total size of the serialized result that a driver can store for each Spark collect action\n",
    "spark.default.parallelism: # Default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set by user. It can be set as spark.executor.instances * spark.executor.cores * 2\n",
    "spark.sql.shuffle.partitions: determine how many partitions are used when data is shuffled between nodes, e.g., joins or aggregations. usually 1~5 times of executor.instances * executor.cores\n",
    "spark.memory.storageFraction: determines the fraction of the heap space that is allocated to caching RDDs and DataFrames in memory.\n",
    "spark.kryoserializer.buffer.max: determine the maximum of data that can be serialized at once; this must be larger than any object we attempt to serialize\n",
    "spark.rpc.message.maxSize: # Maximum message size (in MiB) to allow in \"control plane\" communication; generally only applies to map output size information sent between executors and the driver. To communicate between the nodes, Spark uses a protocol called RPC (Remote Procedure Call), which sends messages back and forth. The spark.rpc.message.maxSize parameter limits how big these messages can be. \n",
    "spark.sql.broadcastTimeout: Spark will wait for this amount of time before giving up on broadcasting a table. Broadcasting can take a long time if the table is large or if there is a shuffle operation before it.\n",
    "spark.sql.autoBroadcastJoinThreshold: Spark will broadcast a table to all worker nodes when performing a join if its size is less than this value; -1 means disabling broadcasting\n",
    "'''\n",
    "\n",
    "date = time.strftime(\"%m,%d,%Y\")\n",
    "date_name = date.split(',')[0] + date.split(',')[1] + date.split(',')[2]\n",
    "\n",
    "hour = time.strftime(\"%H,%M\")\n",
    "hour_name = hour.split(',')[0] + hour.split(',')[1]\n",
    "\n",
    "# set the Spark app name\n",
    "spark_app_name = \"Compute_2-manifolds_\" + tin_filename + '_' + date_name + '_' + hour_name\n",
    "print(\"spark_app_name:\", spark_app_name)\n",
    "\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(spark_app_name) \\\n",
    ".master('yarn') \\\n",
    ".config(\"spark.serializer\", KryoSerializer.getName) \\\n",
    ".config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) \\\n",
    ".config('spark.jars','sedona-core-2.4_2.11-1.0.0-incubating.jar,sedona-sql-2.4_2.11-1.0.0-incubating.jar,sedona-python-adapter-2.4_2.11-1.0.0-incubating.jar,sedona-viz-2.4_2.11-1.0.0-incubating.jar,geotools-wrapper-geotools-24.0.jar,graphframes-0.8.0-spark2.4-s_2.11.jar') \\\n",
    ".config('spark.executor.cores', Num_core_per_executor) \\\n",
    ".config('spark.executor.instances', Num_executor) \\\n",
    ".config('spark.executor.memory', Memory_executor) \\\n",
    ".config('spark.executor.memoryOverhead', MemoryOverhead_executor) \\\n",
    ".config('spark.driver.cores', Num_core_per_driver) \\\n",
    ".config('spark.driver.memory', Memory_driver) \\\n",
    ".config('spark.driver.memoryOverhead', MemoryOverhead_driver) \\\n",
    ".config('spark.driver.maxResultSize', '0') \\\n",
    ".config('spark.dynamicAllocation.enabled', 'false') \\\n",
    ".config('spark.network.timeout', '10000001s') \\\n",
    ".config('spark.executor.heartbeatInterval', '10000000s') \\\n",
    ".config('spark.sql.shuffle.partitions', Num_shuffle_partitions) \\\n",
    ".config(\"spark.default.parallelism\", '200') \\\n",
    ".config(\"spark.kryoserializer.buffer.max\", \"1024mb\") \\\n",
    ".config('spark.rpc.message.maxSize', '256') \\\n",
    ".config(\"spark.sql.broadcastTimeout\", \"36000\") \\\n",
    ".config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    ".config(\"spark.python.profile\", \"true\") \\\n",
    ".config(\"spark.eventLog.enabled\", \"true\") \\\n",
    ".config('spark.yarn.dist.archives', '/local/data/yuehui/py37.tar.gz#environment') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "electronic-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/local/data/yuehui/pyspark/FormanGradient/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iraqi-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphframes\n",
    "from graphframes import GraphFrame\n",
    "from graphframes import *\n",
    "from graphframes.lib import Pregel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mediterranean-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_ver_order() is a function used to read vertices from a csv file\n",
    "def read_ver_order(filtra, directory, tin_filename):\n",
    "    '''\n",
    "    this function has three input parameters.\n",
    "    filtra: 'yes' or 'no', yes means that the input csv file is ordered by default\n",
    "    directory: a string denoting the directory to a TIN file\n",
    "    tin_filename: a string denoting the file name of a TIN without extension, e.g., Canyon_Lake_Gorge_TX\n",
    "    '''\n",
    "    if filtra.lower() == 'yes': # 'yes' means we have already computed the filtra value for each vertex\n",
    "        schema_ver_origin = StructType([ \\\n",
    "            StructField(\"x\",FloatType(),True), \\\n",
    "            StructField(\"y\",FloatType(),True), \\\n",
    "            StructField(\"ele\",FloatType(),True), \\\n",
    "            StructField(\"self_index\",IntegerType(),True), \\\n",
    "            StructField(\"self_order\",IntegerType(),True) \\\n",
    "          ])\n",
    "        \n",
    "        hdfs_tin_pts_origin = directory + \"/\" + tin_filename + '_filtra_pts_origin.csv'\n",
    "    \n",
    "        df_ver_order = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\", False) \\\n",
    "              .schema(schema_ver_origin)\\\n",
    "              .load(hdfs_tin_pts_origin)\n",
    "        print(\"Number of partitions for df_ver_order:\", df_ver_order.rdd.getNumPartitions())\n",
    "        \n",
    "    if filtra.lower() == 'no': # 'no' means we need to rank vertices to get their filtra values\n",
    "        schema_ver_origin = StructType([ \\\n",
    "            StructField(\"x\",FloatType(),True), \\\n",
    "            StructField(\"y\",FloatType(),True), \\\n",
    "            StructField(\"ele\",FloatType(),True), \\\n",
    "            StructField(\"self_index\",IntegerType(),True) \\\n",
    "          ])\n",
    "        \n",
    "        hdfs_tin_pts_origin = directory + \"/\" + tin_filename + '_pts_origin.csv'\n",
    "    \n",
    "        df_ver_origin = spark.read.format(\"csv\") \\\n",
    "              .option(\"header\", False) \\\n",
    "              .schema(schema_ver_origin)\\\n",
    "              .load(hdfs_tin_pts_origin)\n",
    "        \n",
    "        print(\"Number of partitions for df_ver_origin:\", df_ver_origin.rdd.getNumPartitions())\n",
    "        \n",
    "        # define a window for the ordering\n",
    "        # row_number() function along with partitionBy() of other column populates the row number by group\n",
    "        # Since we want to order the whole DataFrame, so we don't need the partitionBy() function\n",
    "        w = Window().orderBy(col('ele').asc())\n",
    "        df_ver_order = df_ver_origin.withColumn(\"self_order\", F.row_number().over(w) -1) # let the row number start from 0\n",
    "        print(\"Number of partitions for df_ver_order:\", df_ver_order.rdd.getNumPartitions())\n",
    "        \n",
    "    return df_ver_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "synthetic-mattress",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_ver_order: 200\n",
      "root\n",
      " |-- x: float (nullable = true)\n",
      " |-- y: float (nullable = true)\n",
      " |-- ele: float (nullable = true)\n",
      " |-- self_index: integer (nullable = true)\n",
      " |-- self_order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read vertices\n",
    "df_ver_order = read_ver_order(filtra, directory, tin_filename)\n",
    "df_ver_order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "armed-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_tri_order() is a function used to read triangles from a csv file\n",
    "def read_tri_order(filtra, directory, tin_filename):\n",
    "    '''\n",
    "    this function has two input parameters.\n",
    "    filtra: 'yes' or 'no', yes means that the input csv file is ordered by default\n",
    "    directory: a string denoting the directory to a TIN file\n",
    "    tin_filename: a string denoting the file name of a tin without extension, e.g., 827_monviso\n",
    "    '''\n",
    "    if filtra.lower() == 'yes':\n",
    "        hdfs_tin_tri_origin = directory + \"/\" + tin_filename + '_filtra_tri_origin.csv'\n",
    "    else: # filtra.lower() == 'no'\n",
    "        hdfs_tin_tri_origin = directory + \"/\" + tin_filename + '_tri_origin.csv'\n",
    "        \n",
    "    schema_tri_origin = StructType([ \\\n",
    "        StructField(\"v1\",IntegerType(),True), \\\n",
    "        StructField(\"v2\",IntegerType(),True), \\\n",
    "        StructField(\"v3\",IntegerType(),True), \\\n",
    "        StructField(\"tri_order\",IntegerType(),True) \\\n",
    "      ])\n",
    "\n",
    "    df_tri_origin = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\", False) \\\n",
    "          .schema(schema_tri_origin)\\\n",
    "          .load(hdfs_tin_tri_origin)\n",
    "    print(\"Number of partitions for df_tri_origin:\", df_tri_origin.rdd.getNumPartitions())\n",
    "        \n",
    "    return df_tri_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "palestinian-albuquerque",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_tri_origin: 200\n",
      "root\n",
      " |-- v1: integer (nullable = true)\n",
      " |-- v2: integer (nullable = true)\n",
      " |-- v3: integer (nullable = true)\n",
      " |-- tri_order: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read triangles\n",
    "df_tri_origin = read_tri_order(filtra, directory, tin_filename)\n",
    "df_tri_origin.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "needed-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_ver() replaces the original index of each vertex with filtra value in df_tri_origin\n",
    "def replace_ver(df_ver_order, df_tri_origin):\n",
    "    '''\n",
    "    df_ver_order: a DataFrame storing sorted vertices with filtra values\n",
    "    df_tri_origin: a DataFrame storing triangles from a TIN\n",
    "    '''\n",
    "    df_tri_order_v1 = df_tri_origin.join(df_ver_order, df_tri_origin.v1 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v1 = df_tri_order_v1.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"),col(\"self_order\").alias(\"r1\"), col(\"ele\").alias(\"r1_ele\"))\n",
    "\n",
    "    df_tri_order_v2 = df_tri_order_v1.join(df_ver_order, df_tri_order_v1.v2 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v2 = df_tri_order_v2.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"), col(\"r1\"), col(\"self_order\").alias(\"r2\"), col(\"r1_ele\"), col(\"ele\").alias(\"r2_ele\"))\n",
    "\n",
    "    df_tri_order_v3 = df_tri_order_v2.join(df_ver_order, df_tri_order_v2.v3 == df_ver_order.self_index, \"inner\")\n",
    "    df_tri_order_v3 = df_tri_order_v3.select(col(\"tri_order\"), col(\"v1\"), col(\"v2\"), col(\"v3\"), col(\"r1\"), col(\"r2\"), col(\"self_order\").alias(\"r3\"), col(\"r1_ele\"), col(\"r2_ele\"), col(\"ele\").alias(\"r3_ele\"))\n",
    "\n",
    "    df_tri_order = df_tri_order_v3.select(col(\"tri_order\"), col(\"r1\"), col(\"r2\"), col(\"r3\"), col(\"r1_ele\"), col(\"r2_ele\"), col(\"r3_ele\"))\n",
    "\n",
    "    return df_tri_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "progressive-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tri_order: integer (nullable = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- r2_ele: float (nullable = true)\n",
      " |-- r3_ele: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replaces original vertex index with filtra values\n",
    "df_tri_order = replace_ver(df_ver_order, df_tri_origin)\n",
    "df_tri_order.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "female-tender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tri_order: integer (nullable = true)\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r2: integer (nullable = true)\n",
      " |-- r3: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- r2_ele: float (nullable = true)\n",
      " |-- r3_ele: float (nullable = true)\n",
      " |-- tri: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- tri_ele: array (nullable = false)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the extreme vertices of a triangle in ascending order, e.g., (2,5,3) -> (2,3,5)\n",
    "\n",
    "'''\n",
    "def get_tri_array(r1, r2, r3):\n",
    "# get_multi_pt_index is used to obtain the adjacent vertexes index, including the vertex itself\n",
    "# pt_list1: partial adjacent vertex indexes of join result 1\n",
    "# pt_list2: partial adjacent vertex indexes of join result 2\n",
    "    tri = [r1, r2, r3]    \n",
    "    tri.sort(reverse=True)\n",
    "    \n",
    "    return tri\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_tri_array_udf = udf(get_tri_array, ArrayType(IntegerType()))\n",
    "df_tri_order = df_tri_order.withColumn(\"tri\", get_tri_array_udf(df_tri_order.r1, df_tri_order.r2, df_tri_order.r3))\n",
    "'''\n",
    "\n",
    "df_tri_order = df_tri_order.withColumn(\"tri_origin\", F.array(\"r1\", \"r2\", \"r3\")).withColumn(\"tri_ele_origin\", F.array(\"r1_ele\", \"r2_ele\", \"r3_ele\"))\n",
    "df_tri_order = df_tri_order.withColumn(\"tri\", sort_array(\"tri_origin\", False)).drop(\"tri_origin\") # sort_array(\"tri\", False), False means descending order\n",
    "df_tri_order = df_tri_order.withColumn(\"tri_ele\", sort_array(\"tri_ele_origin\", False)).drop(\"tri_ele_origin\") # sort_array(\"tri\", False), False means descending order\n",
    "\n",
    "df_tri_order.printSchema()\n",
    "df_tri_order.rdd.getNumPartitions()\n",
    "#df_tri_order.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-suicide",
   "metadata": {},
   "source": [
    "##### globally get VT relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "danish-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union and group vertices and get the preliminary VT relation\n",
    "def grp_union(df_tri_order):\n",
    "    '''\n",
    "    df_tri_order: a DataFrame storing sorted extreme vertices of each triangle\n",
    "    '''\n",
    "    # groupby and collect_set\n",
    "    df_tri_group1 = df_tri_order.groupBy('r1','r1_ele').agg(collect_list('tri').alias('multi_tri'), collect_list('tri_ele').alias('multi_tri_ele'))\n",
    "    df_tri_group2 = df_tri_order.groupBy('r2','r2_ele').agg(collect_list('tri').alias('multi_tri'), collect_list('tri_ele').alias('multi_tri_ele'))\n",
    "    df_tri_group3 = df_tri_order.groupBy('r3', 'r3_ele').agg(collect_list('tri').alias('multi_tri'), collect_list('tri_ele').alias('multi_tri_ele'))\n",
    "    \n",
    "    union12 = df_tri_group1.union(df_tri_group2) # the title of union12 will be the same as df_sj_group1\n",
    "    union123 = union12.union(df_tri_group3) # the title of union123 will be the same as union12, so as df_sj_group1\n",
    "\n",
    "    union123_group = union123.groupBy('r1','r1_ele').agg(collect_list('multi_tri').alias('multi_tri_list'), collect_list('multi_tri_ele').alias('multi_tri_ele_list'))\n",
    "    \n",
    "    # print(\"the schema of union123_group:\", union123_group.printSchema())\n",
    "    return union123_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "practical-hindu",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- r1_ele: float (nullable = true)\n",
      " |-- multi_tri_list: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_tri_ele_list: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union123_group = grp_union(df_tri_order)\n",
    "union123_group.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "liked-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VT_ele: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      "\n",
      "number of partitions for df_VT: 200\n"
     ]
    }
   ],
   "source": [
    "# sort the triangles in the preliminary VT relation\n",
    "def get_multi_tri_order(tri_list, tri_ele_list):\n",
    "    '''\n",
    "    get_multi_tri_order is used to obtain the adjacent triangles, the results are in ascending order, but the triangle itself is in descending\n",
    "    e.g., [[3, 2, 1], [5, 3, 2], [5, 3, 1]]\n",
    "    tri_list: an array of array of array of integer\n",
    "    tri_ele_list: an array of array of array of float\n",
    "    '''\n",
    "\n",
    "    tri = []\n",
    "    tri_ele = []\n",
    "    # pt_list.append(pt_self) # if we don't calculate roughness, we don't need self vertex\n",
    "    for i in range(len(tri_list)):\n",
    "        for j in range(len(tri_list[i])):\n",
    "            # if tri_list[i][j] not in tri: # we do not need to check if it exists in tri_list, because we can prove that each one is unique\n",
    "            tri.append(tri_list[i][j]) # update() will add multiple elements to a set, while add() only add one element to a set   \n",
    "            tri_ele.append(tri_ele_list[i][j])\n",
    "    \n",
    "    tri_copy = copy.deepcopy(tri) # deep copy tri\n",
    "    tri.sort() # sort the list of array, e.g., tri=[[3, 2, 1], [5, 2, 1], [5, 3, 1]], after sorting, [[3, 2, 1], [5, 2, 1], [5, 3, 1]]\n",
    "    \n",
    "    tri_ele_sort = []\n",
    "    # sort tri_ele according to the same rule as tri\n",
    "    for i in range(len(tri)):\n",
    "        index_in_tri_origin = tri_copy.index(tri[i])\n",
    "        tri_ele_sort.append(tri_ele[index_in_tri_origin])\n",
    "    \n",
    "    return tri, tri_ele_sort\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "\n",
    "# StructType for get_multi_tri_order        \n",
    "get_multi_tri_order_schema = StructType([\n",
    "    StructField(\"multi_tri_sort\", ArrayType(ArrayType(IntegerType())),True), \n",
    "    StructField('multi_tri_ele_sort',ArrayType(ArrayType(FloatType())),True)\n",
    "])\n",
    "\n",
    "get_multi_tri_order_udf = udf(get_multi_tri_order, get_multi_tri_order_schema)\n",
    "\n",
    "df_VT = union123_group.withColumn(\"multi_tri_order\", get_multi_tri_order_udf(union123_group.multi_tri_list, union123_group.multi_tri_ele_list))\n",
    "df_VT = df_VT.select(col(\"r1\").alias(\"Ver\"), col(\"multi_tri_order.multi_tri_sort\").alias(\"VT_filtra\"), col(\"multi_tri_order.multi_tri_ele_sort\").alias(\"VT_ele\"))\n",
    "df_VT.printSchema()\n",
    "print(\"number of partitions for df_VT:\", df_VT.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-chuck",
   "metadata": {},
   "source": [
    "##### globally get VV relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "catholic-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get VV using groupby\n",
    "def get_VV(df_tri_order):\n",
    "    df_VV_init_1 = df_tri_order.select(\"r1\", \"r2\", \"r3\")\n",
    "    df_VV_init_2 = df_tri_order.select(\"r2\", \"r1\", \"r3\")\n",
    "    df_VV_init_3 = df_tri_order.select(\"r3\", \"r1\", \"r2\")\n",
    "    \n",
    "    df_VV_union12 = df_VV_init_1.union(df_VV_init_2)\n",
    "    df_VV_union123 = df_VV_union12.union(df_VV_init_3)\n",
    "    \n",
    "    # df_VV_union123.cache()\n",
    "    \n",
    "    # df_VV_union123.persist(StorageLevel.MEMORY_ONLY)\n",
    "    # df_VV_union123.printSchema()\n",
    "    \n",
    "    df_VV_gp = df_VV_union123.groupBy('r1').agg(collect_set('r2').alias('multi_r2_set'),collect_set('r3').alias('multi_r3_set'))\n",
    "    \n",
    "    return df_VV_gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sensitive-signature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_r2_set: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- multi_r3_set: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VV_gp = get_VV(df_tri_order)\n",
    "\n",
    "# df_VV_gp.cache()\n",
    "\n",
    "df_VV_gp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "published-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain VV relation\n",
    "def get_multi_pt_order(pt_list1, pt_list2):\n",
    "# get_multi_pt_index is used to obtain the adjacent vertexes index, including the vertex itself\n",
    "# pt_list1: partial adjacent vertex indexes of join result 1\n",
    "# pt_list2: partial adjacent vertex indexes of join result 2\n",
    "    pt_init = pt_list1 + pt_list2\n",
    "    pt_set = set(pt_init)\n",
    "    \n",
    "    pt_list = sorted(pt_set) # sorted(pt_set, reverse=False), False in ascending order while True in descending order\n",
    "    \n",
    "    return pt_list # directly return a list, e.g., [0, 1, 5, 6], the length of returned column list will be 12, incluing \"[\" and empty char\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_multi_pt_order_udf = udf(get_multi_pt_order, ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "relative-royalty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VV: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VV = df_VV_gp.withColumn(\"VV\", get_multi_pt_order_udf(df_VV_gp.multi_r2_set, df_VV_gp.multi_r3_set)).drop('multi_r2_set', 'multi_r3_set')\n",
    "\n",
    "df_VV = df_VV.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VV.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-level",
   "metadata": {},
   "source": [
    "##### globally get VE relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "serious-casting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- r1: integer (nullable = true)\n",
      " |-- multi_e1: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- multi_e2: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function to get VE from DF_T\n",
    "def get_VE_init(df_tri_order):\n",
    "    df_VE_1 = df_tri_order.withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\"), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\"), False)).drop('r2', 'r3')\n",
    "    df_VE_2 = df_tri_order.withColumn(\"e1\", sort_array(F.array(\"r1\", \"r2\"), False)).withColumn(\"e2\", sort_array(F.array(\"r2\", \"r3\"), False)).drop('r1', 'r3')\n",
    "    df_VE_3 = df_tri_order.withColumn(\"e1\", sort_array(F.array(\"r2\", \"r3\"), False)).withColumn(\"e2\", sort_array(F.array(\"r1\", \"r3\"), False)).drop('r1', 'r2')\n",
    "    \n",
    "    df_VE_union12 = df_VE_1.union(df_VE_2)\n",
    "    df_VE_union123 = df_VE_union12.union(df_VE_3)\n",
    "    \n",
    "    df_VE_init = df_VE_union123.groupBy('r1').agg(collect_set('e1').alias('multi_e1'), collect_set('e2').alias('multi_e2'))\n",
    "    return df_VE_init\n",
    "\n",
    "df_VE_init = get_VE_init(df_tri_order)\n",
    "\n",
    "# df_VE_init.cache()\n",
    "\n",
    "df_VE_init.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pleasant-massachusetts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain VV relation\n",
    "def get_VE(multi_e1, multi_e2):\n",
    "# get_VE is used to obtain the partial VE relation\n",
    "# multi_e1: partial VE relation\n",
    "# multi_e2: partial VE relation\n",
    "    edges = set()\n",
    "    for e in multi_e1:\n",
    "        edges.add(tuple(e))\n",
    "        \n",
    "    for e in multi_e2:\n",
    "        edges.add(tuple(e))\n",
    "    \n",
    "    edges_list = sorted(edges) # sorted(pt_set, reverse=False), False in ascending order while True in descending order\n",
    "    \n",
    "    return edges_list\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_VE_udf = udf(get_VE, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_VE = df_VE_init.withColumn(\"VE\", get_VE_udf(df_VE_init.multi_e1, df_VE_init.multi_e2)).drop('multi_e1', 'multi_e2')\n",
    "df_VE = df_VE.withColumnRenamed('r1', 'Ver')\n",
    "\n",
    "df_VE.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "falling-repair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VT_ele: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- VV: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VV = df_VT.join(df_VV, df_VT.Ver == df_VV.Ver)\n",
    "df_VT_VV = df_VT_VV.select(df_VT.Ver, \"VT_filtra\", \"VT_ele\", \"VV\")\n",
    "\n",
    "df_VT_VV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "pacific-primary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VT_ele: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- VV: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- VE: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_VT_VV_VE = df_VT_VV.join(df_VE, df_VT_VV.Ver == df_VE.Ver)\n",
    "df_VT_VV_VE = df_VT_VV_VE.select(df_VT_VV.Ver, \"VT_filtra\", \"VT_ele\", \"VV\", \"VE\")\n",
    "\n",
    "df_VT_VV_VE.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-trading",
   "metadata": {},
   "source": [
    "##### compute the lower star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "documentary-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_edge: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_tri_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- LS_tri_ele: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain Lower Star\n",
    "def get_LS(Ver, VT_filtra, VT_ele, VV):\n",
    "    # get_LS is used to obtain the VV and VE relation and compute the lower star\n",
    "    \n",
    "    # VE relation\n",
    "    LS_edge = []\n",
    "    for i in range(len(VV)):\n",
    "        if VV[i] < int(Ver):\n",
    "            LS_edge.append([int(Ver),VV[i]])\n",
    "        \n",
    "    LS_tri = []\n",
    "    LS_tri_ele = []\n",
    "    \n",
    "    for i in range(len(VT_filtra)):\n",
    "        if Ver >= max(VT_filtra[i]): # Ver_ele is not the maximum vertex\n",
    "            LS_tri.append(VT_filtra[i])\n",
    "            LS_tri_ele.append(VT_ele[i])\n",
    "                \n",
    "    return LS_edge, LS_tri, LS_tri_ele\n",
    "\n",
    "# StructType for get_multi_tri_order        \n",
    "get_LS_schema = StructType([\n",
    "    StructField(\"LS_edge\", ArrayType(ArrayType(IntegerType())),True), \n",
    "    StructField(\"LS_tri_filtra\", ArrayType(ArrayType(IntegerType())),True),\n",
    "    StructField('LS_tri_ele',ArrayType(ArrayType(FloatType())),True)\n",
    "])\n",
    "\n",
    "get_LS_udf = udf(get_LS, get_LS_schema)\n",
    "\n",
    "df_LS = df_VT_VV_VE.withColumn(\"LS\", get_LS_udf(df_VT_VV_VE.Ver, df_VT_VV_VE.VT_filtra, df_VT_VV_VE.VT_ele, df_VT_VV_VE.VV))\n",
    "df_LS = df_LS.select(\"Ver\", \"VT_filtra\", col(\"LS.LS_edge\").alias(\"LS_edge\"), col(\"LS.LS_tri_filtra\").alias(\"LS_tri_filtra\"), col(\"LS.LS_tri_ele\").alias(\"LS_tri_ele\"))\n",
    "df_LS.printSchema()\n",
    "# df_LS_edge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-boxing",
   "metadata": {},
   "source": [
    "### compute Forman Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "shared-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "concrete-physiology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the Forman gradient from lower stars\n",
    "def get_Forman(Ver, LS_edge, LS_filtra, LS_ele):\n",
    "    '''\n",
    "    get_Forman is used to compute Forman gradient\n",
    "    LS_edge is triangles of lower star in ascending order\n",
    "    LS_tri is triangles of lower star in ascending order\n",
    "    '''\n",
    "    # inner udf to get the number of unpaired edges for a triangle\n",
    "    def num_unpaired_edges(tri_inner, crit_cell_inner, Forman_vec_pair_VE_inner, Forman_vec_pair_ET_inner):\n",
    "        # compute the number of unpaired edges\n",
    "        edge0 = [tri_inner[0], tri_inner[1]]\n",
    "        edge1 = [tri_inner[0], tri_inner[2]]\n",
    "        \n",
    "        num_unpaired_edge = 2\n",
    "        paired_edge = []\n",
    "        for crit_inner_temp in crit_cell_inner:\n",
    "            if len(crit_inner_temp) == 2: # crit_inner_temp is a critical edge\n",
    "                paired_edge.append(crit_inner_temp)\n",
    "                \n",
    "        for pair_vec in Forman_vec_pair_VE_inner: # pair_vec is a vector, it can be (ver, edge) or (edge, tri)\n",
    "            paired_edge.append(pair_vec[1])\n",
    "            # if type(pair_vec[0]) != int and len(pair_vec[0]) == 2: # pair_vec[0] is a paired edge, like in (edge, tri)\n",
    "                # paired_edge.append(pair_vec[0])\n",
    "            # if len(pair_vec[1]) == 2: # pair_vec[1] is a paired edge, like in (ver, edge)\n",
    "                # paired_edge.append(pair_vec[1])\n",
    "        for pair_vec in Forman_vec_pair_ET_inner: # pair_vec is a vector, it can be (ver, edge) or (edge, tri)\n",
    "            paired_edge.append(pair_vec[0])\n",
    "                \n",
    "        if edge0 in paired_edge:\n",
    "            num_unpaired_edge = num_unpaired_edge - 1\n",
    "        if edge1 in paired_edge:\n",
    "            num_unpaired_edge = num_unpaired_edge - 1\n",
    "            \n",
    "        return num_unpaired_edge\n",
    "    \n",
    "    # inner udf to get the edge which is paired with a triangles\n",
    "    def pair_an_edge_from_PQzero(alpha_inner, PQzero_inner):\n",
    "    # alpha_inner is a triangle in [r0, r1, r2] format, and the vertext's elevation is in descending order\n",
    "    # PQzero_inner is a queue storing all other edges\n",
    "        alpha_e0 = [alpha_inner[0], alpha_inner[1]]\n",
    "        alpha_e1 = [alpha_inner[0], alpha_inner[2]]\n",
    "        for edge_inner in PQzero_inner:\n",
    "            if edge_inner == alpha_e0:\n",
    "                return edge_inner\n",
    "            if edge_inner == alpha_e1:\n",
    "                return edge_inner\n",
    "            \n",
    "    crit_cell = [] # crit_cell will store critical simplices\n",
    "    # Forman_vec_pair = [] # Forman_vec_pair will store all Forman gradient pairs as a vector\n",
    "    Forman_vec_pair_VE = []\n",
    "    Forman_vec_pair_ET = []\n",
    "    if (len(LS_edge)+len(LS_filtra)) == 0: # ver is a local minimum\n",
    "        crit_cell.append([Ver]) # store minimum as an array [Ver] though it has only one element\n",
    "    elif len(LS_edge) > 0:\n",
    "        Forman_vec_pair_VE.append([Ver, LS_edge[0]]) # LS_edge[0] is the minimum edge since it is in ascending order\n",
    "        \n",
    "        # define two queues, PQzero storing all other edges, PQone storing all triangles which have only one unpaired edge\n",
    "        PQzero = deque()\n",
    "        PQone = deque()\n",
    "        PQone_ele = deque()\n",
    "        for i in range(1, len(LS_edge)): # add the other edges to PQzero\n",
    "            PQzero.append(LS_edge[i])\n",
    "            \n",
    "        cell_temp_index_of_remove = []\n",
    "        index_temp = 0\n",
    "        for cell_temp in LS_filtra:\n",
    "            if num_unpaired_edges(cell_temp, crit_cell, Forman_vec_pair_VE, Forman_vec_pair_ET) == 1:\n",
    "                PQone.append(cell_temp) # storing cells with one unpaired edges, which are triangles in TIN\n",
    "                PQone_ele.append(LS_ele[index_temp])\n",
    "                # remove cell_temp from LS_filtra\n",
    "                # LS_filtra.remove(cell_temp)\n",
    "                cell_temp_index_of_remove.append(index_temp)\n",
    "            index_temp += 1\n",
    "            \n",
    "        # remove cell_temp from LS_filtra and LS_ele\n",
    "        for i in reversed(cell_temp_index_of_remove):\n",
    "            del LS_filtra[i]\n",
    "            del LS_ele[i]\n",
    "                \n",
    "        # sort PQone in ascending order, the initial queue is already sorted when creating\n",
    "        # PQone = sorted(PQone) # sort() is not supported but we can use sorted()\n",
    "        # PQone = deque(PQone) # after sorting, PQone one will be a list instead of a queue, we need to reconstruct this queue\n",
    "        while len(PQone) > 0 or len(PQzero) > 0:\n",
    "            while len(PQone) > 0:\n",
    "                # alpha = PQone.popleft() # alpha is a triangle in the PQone\n",
    "                # alpha should be a triangle with lower elevation when PQone have more than one triangle\n",
    "                if len(PQone) > 1 and abs(PQone_ele[0][1]-PQone_ele[1][1]) < 0.000001 and PQone_ele[0][2] > PQone_ele[1][2]:\n",
    "                    alpha = PQone[1] # alpha is the second triangle\n",
    "                    del PQone[1]\n",
    "                    del PQone_ele[1]\n",
    "                else:\n",
    "                    alpha = PQone.popleft() # alpha is the first triangle, which is the same as alpha=PQone[0], then del PQone[0]\n",
    "                    del PQone_ele[0]\n",
    "                \n",
    "                if num_unpaired_edges(alpha, crit_cell, Forman_vec_pair_VE, Forman_vec_pair_ET) == 0:\n",
    "                    PQzero.append(alpha)\n",
    "                else:\n",
    "                    pair_alpha = pair_an_edge_from_PQzero(alpha, PQzero) # pair_alpha is an edge, [pair_alpha, alpha] will be a new paired vector\n",
    "                    # if pair_alpha in PQzero:\n",
    "                    PQzero.remove(pair_alpha) # remove pair_alpha from PQzero\n",
    "                    # Forman_vec_pair.append([pair_alpha, alpha]) # add the [pair_alpha, alpha], which is [edge, tri]\n",
    "                    Forman_vec_pair_ET.append([pair_alpha, alpha]) # add the [pair_alpha, alpha], which is [edge, tri]\n",
    "                    \n",
    "                    beta_cell_temp_index_of_remove = []\n",
    "                    beta_index_temp = 0\n",
    "                    for cell_beta in LS_filtra: # this can be optimized\n",
    "                        if cell_beta not in PQone and num_unpaired_edges(cell_beta, crit_cell, Forman_vec_pair_VE, Forman_vec_pair_ET) == 1:\n",
    "                            PQone.append(cell_beta)\n",
    "                            PQone_ele.append(LS_ele[beta_index_temp])\n",
    "                            # remove cell_beta from LS_tri\n",
    "                            # LS_tri.remove(cell_beta)\n",
    "                            beta_cell_temp_index_of_remove.append(beta_index_temp)\n",
    "                        beta_index_temp += 1\n",
    "                    \n",
    "                    # remove cell_beta from LS_filtra and LS_ele\n",
    "                    for i in reversed(beta_cell_temp_index_of_remove):\n",
    "                        del LS_filtra[i]\n",
    "                        del LS_ele[i]\n",
    "                    \n",
    "                    PQone = sorted(PQone)\n",
    "                    PQone = deque(PQone) # after sorting, PQone one will be a list instead of a queue, we need to reconstruct this queue\n",
    "            if len(PQzero) > 0:\n",
    "                gama = PQzero.popleft() # gama is a critical simplex\n",
    "                crit_cell.append(gama)\n",
    "                \n",
    "                gama_cell_temp_index_of_remove = []\n",
    "                gama_index_temp = 0\n",
    "                for cell_gama in LS_filtra: # this can be optimized\n",
    "                    if cell_gama not in PQone and num_unpaired_edges(cell_gama, crit_cell, Forman_vec_pair_VE, Forman_vec_pair_ET) == 1:\n",
    "                        PQone.append(cell_gama)\n",
    "                        PQone_ele.append(LS_ele[gama_index_temp])\n",
    "                        # remove cell_gama from LS_tri\n",
    "                        # LS_tri.remove(cell_gama)\n",
    "                        gama_cell_temp_index_of_remove.append(gama_index_temp)\n",
    "                    gama_index_temp += 1\n",
    "                    \n",
    "                # remove cell_beta from LS_filtra and LS_ele\n",
    "                for i in reversed(gama_cell_temp_index_of_remove):\n",
    "                    del LS_filtra[i]\n",
    "                    del LS_ele[i]\n",
    "                        \n",
    "                PQone = sorted(PQone)\n",
    "                PQone = deque(PQone) # after sorting, PQone one will be a list instead of a queue, we need to reconstruct this queue\n",
    "                \n",
    "    return crit_cell, Forman_vec_pair_VE, Forman_vec_pair_ET\n",
    "\n",
    "# StructType for Crit_cell        \n",
    "add_Crit_cell_schema = StructType([\n",
    "    StructField('Crit_cell',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# StructType for VE pairs        \n",
    "add_VE_schema = StructType([\n",
    "    StructField(\"VE_pair_V\", IntegerType(),True), \n",
    "    StructField('VE_pair_E',ArrayType(IntegerType()),True)\n",
    "])\n",
    "\n",
    "# StructType for ET pairs        \n",
    "add_multi_ET_schema = StructType([\n",
    "    StructField(\"ET_pair_E\", ArrayType(IntegerType()),True), \n",
    "    StructField('ET_pair_T',ArrayType(IntegerType()),True)\n",
    "])\n",
    " \n",
    "# the whole StructType\n",
    "get_Forman_schema = StructType([\n",
    "    StructField(\"Crit_cell\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"VE_pair\", ArrayType(add_VE_schema), True),\n",
    "    StructField(\"ET_pair\", ArrayType(add_multi_ET_schema), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_Forman_udf = udf(get_Forman, get_Forman_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-dealer",
   "metadata": {},
   "source": [
    "### OR get Forman gradient from previously saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "north-season",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- Forman: struct (nullable = true)\n",
      " |    |-- Crit_cell: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- VE_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- VE_pair_V: integer (nullable = true)\n",
      " |    |    |    |-- VE_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- ET_pair: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- ET_pair_E: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |    |-- ET_pair_T: array (nullable = true)\n",
      " |    |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_df_Forman = directory + '/' + tin_filename + '_sortForman' + '.parquet'\n",
    "df_Forman = spark.read.parquet(file_df_Forman)\n",
    "df_Forman.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-physiology",
   "metadata": {},
   "source": [
    "# Extract critical vertices, edeges, and triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spatial-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for df_crit: 200\n",
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Critical: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crit = df_Forman.select(\"Ver\", col(\"Forman.Crit_cell\").alias(\"Critical\"))\n",
    "print(\"number of partitions for df_crit:\", df_crit.rdd.getNumPartitions())\n",
    "df_crit.printSchema()\n",
    "# df_crit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "destroyed-elite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for df_crit_VET: 200\n",
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Critical: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- crit_cell: struct (nullable = true)\n",
      " |    |-- crit_ver: integer (nullable = true)\n",
      " |    |-- crit_edge: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |-- crit_tri: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the Critical column to obtain the critical vertices, edges, and triangles\n",
    "def get_crit_VET(crit_cell):\n",
    "    if len(crit_cell) < 1: # crit_cell is empty, there are no critical simplices within this lower star\n",
    "        return\n",
    "    else:\n",
    "        # initialize the critical vertices, critical edges, and critical triangles\n",
    "        crit_ver = None\n",
    "        crit_edge = []\n",
    "        crit_tri = []\n",
    "        for icrit_cell_temp in crit_cell:\n",
    "            if len(icrit_cell_temp) == 1: # icrit_cell_temp stores a critical vertex\n",
    "                crit_ver = icrit_cell_temp[0]\n",
    "            if len(icrit_cell_temp) == 2: # icrit_cell_temp stores critical edges\n",
    "                crit_edge.append(icrit_cell_temp)\n",
    "            if len(icrit_cell_temp) == 3: # icrit_cell_temp stores critical triangles\n",
    "                crit_tri.append(icrit_cell_temp)\n",
    "        if len(crit_edge) == 0:\n",
    "            crit_edge = None\n",
    "        if len(crit_tri) == 0:\n",
    "            crit_tri = None\n",
    "        return crit_ver, crit_edge, crit_tri\n",
    "        \n",
    "# the whole StructType\n",
    "get_crit_VET_schema = StructType([\n",
    "    StructField(\"crit_ver\", IntegerType(), True),\n",
    "    StructField(\"crit_edge\", ArrayType(ArrayType(IntegerType())), True),\n",
    "    StructField(\"crit_tri\", ArrayType(ArrayType(IntegerType())), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_crit_VET_udf = udf(get_crit_VET, get_crit_VET_schema)\n",
    "\n",
    "df_crit_VET = df_crit.withColumn(\"crit_cell\", get_crit_VET_udf(df_crit.Critical))\n",
    "print(\"number of partitions for df_crit_VET:\", df_crit_VET.rdd.getNumPartitions())\n",
    "df_crit_VET.printSchema()\n",
    "# df_crit_VET.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "crazy-click",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Min_ver: integer (nullable = true)\n",
      "\n",
      "number of partitions for df_crit_V: 200\n"
     ]
    }
   ],
   "source": [
    "# to get critical vertices\n",
    "df_crit_V = df_crit_VET.select(col(\"Ver\"), col(\"crit_cell.crit_ver\").alias(\"Min_ver\")).na.drop()\n",
    "df_crit_V.printSchema()\n",
    "print(\"number of partitions for df_crit_V:\", df_crit_V.rdd.getNumPartitions())\n",
    "# df_crit_V.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "loose-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_crit_V = df_crit_V.count()\n",
    "# print(\"number of critical vertices:\", num_crit_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "close-president",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Saddle_edge: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "number of partitions for df_crit_E: 200\n"
     ]
    }
   ],
   "source": [
    "# to get critical edges\n",
    "df_crit_E_init = df_crit_VET.select(col(\"Ver\"), col(\"crit_cell.crit_edge\").alias(\"Saddle_edge_init\")).na.drop()\n",
    "\n",
    "# there may be multiple saddles inside one lower star\n",
    "df_crit_E = df_crit_E_init.select(col(\"Ver\"), explode(\"Saddle_edge_init\").alias(\"Saddle_edge\")).na.drop()\n",
    "df_crit_E.printSchema()\n",
    "print(\"number of partitions for df_crit_E:\", df_crit_E.rdd.getNumPartitions())\n",
    "# df_crit_E.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "impressive-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_crit_E = df_crit_E.count()\n",
    "# print(\"number of critical edges:\", num_crit_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "continued-medium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Max_tri: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "number of partitions for df_crit_T: 200\n"
     ]
    }
   ],
   "source": [
    "# to get critical triangles\n",
    "df_crit_T_init = df_crit_VET.select(col(\"Ver\"), col(\"crit_cell.crit_tri\").alias(\"Max_tri_init\")).na.drop()\n",
    "\n",
    "# there could be multiple critical triangles within one lower star\n",
    "df_crit_T = df_crit_T_init.select(col(\"Ver\"), explode(\"Max_tri_init\").alias(\"Max_tri\")).na.drop()\n",
    "df_crit_T.printSchema()\n",
    "print(\"number of partitions for df_crit_T:\", df_crit_T.rdd.getNumPartitions())\n",
    "# df_crit_T.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "worthy-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_crit_T = df_crit_T.count()\n",
    "# print(\"number of critical triangles:\", num_crit_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-wrist",
   "metadata": {},
   "source": [
    "# Methods to build Graph VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "continued-dietary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Crit_cell: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- VE_pair: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- VE_pair_V: integer (nullable = true)\n",
      " |    |    |-- VE_pair_E: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to obtain critical simplices and VE_paris from the Forman gradient\n",
    "df_Forman_VE_pairs = df_Forman.select(col(\"Ver\"), col(\"Forman.Crit_cell\").alias(\"Crit_cell\"), col(\"Forman.VE_pair\").alias(\"VE_pair\"))\n",
    "df_Forman_VE_pairs.printSchema()\n",
    "# df_Forman_VE_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "heavy-commonwealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# construct a graph from VE_pairs and critical simplices\\ndef Build_V1(VE_pair):\\n    if len(VE_pair) > 0:\\n        return VE_pair[0][1] # VE_pair[0] is a VE pair, VE_pair[0][1] is an edge\\n    \\n# convert a function to an udf and determine the return type\\n# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\\nBuild_V1_udf = udf(Build_V1, ArrayType(IntegerType()))\\ndf_graph_V1 = df_Forman_VE_pairs.withColumn(\"V1_edge\", Build_V1_udf(df_Forman_VE_pairs.VE_pair)).select(col(\"Ver\").alias(\"V1_node\"), \"V1_edge\")\\ndf_graph_V1.printSchema()\\n# df_graph_V1.show()\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# construct a graph from VE_pairs and critical simplices\n",
    "def Build_V1(VE_pair):\n",
    "    if len(VE_pair) > 0:\n",
    "        return VE_pair[0][1] # VE_pair[0] is a VE pair, VE_pair[0][1] is an edge\n",
    "    \n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "Build_V1_udf = udf(Build_V1, ArrayType(IntegerType()))\n",
    "df_graph_V1 = df_Forman_VE_pairs.withColumn(\"V1_edge\", Build_V1_udf(df_Forman_VE_pairs.VE_pair)).select(col(\"Ver\").alias(\"V1_node\"), \"V1_edge\")\n",
    "df_graph_V1.printSchema()\n",
    "# df_graph_V1.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "rubber-sunglasses",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- V1_node: integer (nullable = true)\n",
      " |-- V1_edge: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to get the DataFrame containg all vertices and VE pairs \n",
    "df_graph_V1 = df_Forman_VE_pairs.select(col(\"Ver\").alias(\"V1_node\"), df_Forman_VE_pairs.VE_pair.VE_pair_E.alias(\"V1_edge\"))\n",
    "df_graph_V1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-billion",
   "metadata": {},
   "source": [
    "#### prepare nodes of the Graph VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "greater-morgan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_graph_V1_node = df_graph_V1.select(col(\"V1_node\").alias(\"id\"))\n",
    "df_graph_V1_node.printSchema()\n",
    "# df_graph_V1_node.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-breath",
   "metadata": {},
   "source": [
    "##### repartition and save df_graph_V1_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "black-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost to save Graph_VE nodes in Parquet: 0.00010037422180175781\n"
     ]
    }
   ],
   "source": [
    "# df_graph_V1_node = df_graph_V1_node.repartition(20) date_name\n",
    "t_save_node_0 = time.time()\n",
    "\n",
    "file_df_graph_V1_node = 'SigSpatial_data' + '/' + tin_filename + '_VE_node' + '_' + date_name + '.parquet'\n",
    "# df_graph_V1_node.write.parquet(file_df_graph_V1_node) \n",
    "\n",
    "t_save_node_1 = time.time()\n",
    "print(\"Time cost to save Graph_VE nodes in Parquet:\", t_save_node_1 - t_save_node_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-intake",
   "metadata": {},
   "source": [
    "#### prepare arcs of the Graph VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ignored-webmaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      "\n",
      "number of partitions for df_graph_V1_arc: 200\n"
     ]
    }
   ],
   "source": [
    "df_graph_V1_arc = df_graph_V1.select(df_graph_V1.V1_edge[0][0].alias('src'), df_graph_V1.V1_edge[0][1].alias('dst')).na.drop()\n",
    "df_graph_V1_arc.printSchema()\n",
    "print(\"number of partitions for df_graph_V1_arc:\", df_graph_V1_arc.rdd.getNumPartitions())\n",
    "# df_graph_V1_arc.show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-desire",
   "metadata": {},
   "source": [
    "##### repartition and save the dataframe df_graph_V1_arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "juvenile-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost to save Graph_VE arcs in Parquet: 9.1552734375e-05\n"
     ]
    }
   ],
   "source": [
    "# df_graph_V1_arc = df_graph_V1_arc.repartition(20)\n",
    "# print(\"number of partitions for df_graph_V1_arc:\", df_graph_V1_arc.rdd.getNumPartitions())\n",
    "\n",
    "# save Graph_VE arcs to Parquet\n",
    "t_save_arc_0 = time.time()\n",
    "file_df_graph_V1_arc = 'SigSpatial_data' + '/' + tin_filename + '_VE_arc' + '_' + date_name + '.parquet'\n",
    "# df_graph_V1_arc.write.parquet(file_df_graph_V1_arc) \n",
    "\n",
    "t_save_arc_1 = time.time()\n",
    "print(\"Time cost to save Graph_VE arcs in Parquet:\", t_save_arc_1 - t_save_arc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-heather",
   "metadata": {},
   "source": [
    "##### OR get nodes and arcs from previously saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "unlike-tongue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_graph_V1_node before repartition: 80\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "Number of partitions for df_graph_V1_arc before repartition: 120\n",
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read nodes of V1\n",
    "schema_df_graph_V1_node_final = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True)\n",
    "  ])\n",
    "file_df_graph_V1_node_final_hdfs = 'SigSpatial_data' + '/' + tin_filename + '_graph_V1_node.parquet'\n",
    "df_graph_V1_node = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_graph_V1_node_final)\\\n",
    "      .load(file_df_graph_V1_node_final_hdfs)\n",
    "print(\"Number of partitions for df_graph_V1_node before repartition:\", df_graph_V1_node.rdd.getNumPartitions())\n",
    "df_graph_V1_node.printSchema()\n",
    "\n",
    "# read arcs of V1\n",
    "schema_df_graph_V1_edge_final = StructType([ \\\n",
    "    StructField(\"src\",IntegerType(),True), \\\n",
    "    StructField(\"dst\",IntegerType(),True)\n",
    "  ])\n",
    "file_df_graph_V1_edge_final_hdfs = 'SigSpatial_data' + '/' + tin_filename + '_graph_V1_arc.parquet'\n",
    "df_graph_V1_arc = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_graph_V1_edge_final)\\\n",
    "      .load(file_df_graph_V1_edge_final_hdfs)\n",
    "print(\"Number of partitions for df_graph_V1_arc before repartition:\", df_graph_V1_arc.rdd.getNumPartitions())\n",
    "df_graph_V1_arc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-omaha",
   "metadata": {},
   "source": [
    "### construct Graph_VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "satellite-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_VE = GraphFrame(df_graph_V1_node, df_graph_V1_arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "final-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a checkpoint directory to improve performance\n",
    "# Checkpointing regularly helps recover from failures, clean shuffle files, shorten the\n",
    "# lineage of the computation graph, and reduce the complexity of plan optimization.\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-ready",
   "metadata": {},
   "source": [
    "### Compute influence regions of minima, which are the connected components of Graph_VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "victorian-provincial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n",
      "time cost of connected components: 149.9858365058899\n"
     ]
    }
   ],
   "source": [
    "t_con_0 = time.time()\n",
    "# result_con consists of two columns, vertex id, component\n",
    "result_con = graph_VE.connectedComponents()\n",
    "result_con.printSchema()\n",
    "\n",
    "t_con_1 = time.time()\n",
    "t_con = t_con_1 - t_con_0\n",
    "print(\"time cost of connected components:\",t_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-paste",
   "metadata": {},
   "source": [
    "##### 1) left join result_con with saddles to identify the boundary vertices of saddles that belong to the same component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "configured-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Saddle_pts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain the saddle pts\n",
    "df_crit_E2pts = df_crit_E.select(explode(\"Saddle_edge\").alias(\"Saddle_pts\"))\n",
    "# some nodes may belong to multiple saddles, we need to remove them\n",
    "df_crit_E2pts = df_crit_E2pts.distinct()\n",
    "df_crit_E2pts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "improving-observation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Saddle_pts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR get df_crit_E2pts from previously saved results\n",
    "\n",
    "schema_df_crit_E2pts = StructType([ \\\n",
    "    StructField(\"Saddle_pts\",IntegerType(),True)\n",
    "  ])\n",
    "file_df_crit_E2pts_hdfs = 'hdfs_data' + '/' + tin_filename + '_crit_E2pts.parquet'\n",
    "df_crit_E2pts_file = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_crit_E2pts)\\\n",
    "      .load(file_df_crit_E2pts_hdfs)\n",
    "\n",
    "df_crit_E2pts_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "stainless-husband",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      " |-- att: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# broadcast left join connected components with saddles\n",
    "# result_con contains the df_graph_V1_node\n",
    "result_con_saddle = result_con.join(df_crit_E2pts,result_con.id==df_crit_E2pts.Saddle_pts, \"inner\")\n",
    "result_con_saddle = result_con_saddle.withColumn(\"att\", when(result_con_saddle.Saddle_pts.isNull() == False, result_con_saddle.Saddle_pts).otherwise(-1)).drop('Saddle_pts')\n",
    "\n",
    "unique_SdlPts_per_con = result_con_saddle.groupBy('component').agg(collect_list('id').alias('multi_SdlPts'))\n",
    "\n",
    "result_con_saddle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-kitchen",
   "metadata": {},
   "source": [
    "##### 2) left join df_graph_V1_arc with result_con to identify the arcs that belong to the connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "sustained-barcelona",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join df_graph_V1_arc with result_con to identify which component that each arc belongs to\n",
    "df_G_V1_arc_component = df_graph_V1_arc.join(result_con,df_graph_V1_arc.src==result_con.id, \"left\").drop('id')\n",
    "df_G_V1_arc_component.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "divine-possible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost to find sub_graph_list: 6.743970632553101\n"
     ]
    }
   ],
   "source": [
    "Num_subgraphs = 20\n",
    "t_find_minmax_0 = time.time()\n",
    "\n",
    "# Find the minimum and maximum values of \"component\" in result_con\n",
    "sub_graph_tmp_min = result_con.agg({\"component\": \"min\"}).collect()[0][0]\n",
    "sub_graph_tmp_max = result_con.agg({\"component\": \"max\"}).collect()[0][0]\n",
    "\n",
    "sub_graph_list = []\n",
    "for i in range(Num_subgraphs+1):\n",
    "    sub_graph_tmp = int(i * (sub_graph_tmp_max - sub_graph_tmp_min)/(Num_subgraphs) + sub_graph_tmp_min)\n",
    "    sub_graph_list.append(sub_graph_tmp)\n",
    "    \n",
    "'''\n",
    "\n",
    "# sort and collect result_con_unique\n",
    "result_con_unique = result_con.groupBy('component').count()\n",
    "# result_con_unique.printSchema()\n",
    "# result_con_unique.show()\n",
    "\n",
    "result_con_unique = result_con_unique.sort(col('component'))\n",
    "result_con_unique_col = result_con_unique.collect()\n",
    "\n",
    "sub_graph_list = []\n",
    "\n",
    "for i in range(Num_subgraphs):\n",
    "    tmp = int(i / (Num_subgraphs-1) * (len(result_con_unique_col) -1))\n",
    "    sub_graph_tmp = result_con_unique_col[tmp]['component']\n",
    "    sub_graph_list.append(sub_graph_tmp)\n",
    "'''    \n",
    "\n",
    "t_find_minmax_1 = time.time()\n",
    "t_find_minmax = t_find_minmax_1 - t_find_minmax_0\n",
    "print(\"time cost to find sub_graph_list:\", t_find_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "located-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame with a schema\n",
    "schema_special = StructType([\n",
    "    StructField('id', IntegerType(),True),\n",
    "    StructField(\"component\", IntegerType(),True),\n",
    "    StructField(\"att\", IntegerType(),True),   \n",
    "    StructField(\"label\", IntegerType(),True)   \n",
    "])\n",
    "\n",
    "df_empty = spark.createDataFrame([], schema_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "altered-uniform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for LPA to subgraph 0: 162.915565\n",
      "Time cost for LPA to subgraph 2449372: 155.676687\n",
      "Time cost for LPA to subgraph 4898744: 154.333997\n",
      "Time cost for LPA to subgraph 7348116: 153.238565\n",
      "Time cost for LPA to subgraph 9797489: 143.981099\n",
      "Time cost for LPA to subgraph 12246861: 150.578045\n",
      "Time cost for LPA to subgraph 14696233: 147.890140\n",
      "Time cost for LPA to subgraph 17145605: 153.646286\n",
      "Time cost for LPA to subgraph 19594978: 146.937313\n",
      "Time cost for LPA to subgraph 22044350: 141.634086\n",
      "Time cost for LPA to subgraph 24493722: 145.904270\n",
      "Time cost for LPA to subgraph 26943094: 148.900878\n",
      "Time cost for LPA to subgraph 29392467: 150.281316\n",
      "Time cost for LPA to subgraph 31841839: 151.051762\n",
      "Time cost for LPA to subgraph 34291211: 148.991852\n",
      "Time cost for LPA to subgraph 36740583: 148.250391\n",
      "Time cost for LPA to subgraph 39189956: 144.659430\n",
      "Time cost for LPA to subgraph 41639328: 144.304403\n",
      "Time cost for LPA to subgraph 44088700: 144.043732\n",
      "Time cost for LPA to subgraph 46538072: 152.344347\n",
      "*******************************************************\n",
      "Total time cost for LPA: 2990.0800545215607\n"
     ]
    }
   ],
   "source": [
    "# construct subgraphs and union the LPA results\n",
    "t_LPA_total_0 = time.time()\n",
    "\n",
    "sub_time = [] # to store time cost for each subgraph\n",
    "for i in range(len(sub_graph_list) - 1):\n",
    "    t_LPA_sub_0 = time.time()\n",
    "    sub_graph_tmp_0 = sub_graph_list[i]\n",
    "    sub_graph_tmp_1 = sub_graph_list[i+1]\n",
    "    \n",
    "    if i == len(sub_graph_list) - 1: # the last subgraphs should contain sub_graph_tmp_max\n",
    "        sub_graph_node = result_con_saddle.filter((result_con_saddle.component >= sub_graph_tmp_0) & (result_con_saddle.component <= sub_graph_tmp_1))\n",
    "        sub_graph_arc = df_G_V1_arc_component.filter((df_G_V1_arc_component.component >= sub_graph_tmp_0) & (df_G_V1_arc_component.component <= sub_graph_tmp_1)).drop('id')\n",
    "    else:\n",
    "        sub_graph_node = result_con_saddle.filter((result_con_saddle.component >= sub_graph_tmp_0) & (result_con_saddle.component < sub_graph_tmp_1))\n",
    "        # sub_graph_node.printSchema() # 3 columns: id, component, att\n",
    "        sub_graph_arc = df_G_V1_arc_component.filter((df_G_V1_arc_component.component >= sub_graph_tmp_0) & (df_G_V1_arc_component.component < sub_graph_tmp_1)).drop('id')\n",
    "        # sub_graph_arc.printSchema() # 3 columns: src, dst, component\n",
    "    \n",
    "    sub_graph = GraphFrame(sub_graph_node, sub_graph_arc)\n",
    "    \n",
    "    sub_graph_LPA = sub_graph.pregel \\\n",
    "         .setMaxIter(10) \\\n",
    "         .withVertexColumn(\"label\", col(\"att\"), \\\n",
    "             Pregel.msg()) \\\n",
    "         .sendMsgToDst(when(Pregel.dst(\"label\")>0, Pregel.dst(\"label\")).otherwise(Pregel.src(\"label\"))) \\\n",
    "         .aggMsgs(F.max(Pregel.msg())) \\\n",
    "         .run()\n",
    "    t_LPA_sub_1 = time.time()\n",
    "    df_empty = df_empty.union(sub_graph_LPA)\n",
    "    print(\"Time cost for LPA to subgraph %ld: %f\" % (sub_graph_tmp_0, t_LPA_sub_1 - t_LPA_sub_0))\n",
    "    sub_time.append(t_LPA_sub_1 - t_LPA_sub_0)\n",
    "    \n",
    "t_LPA_total_1 = time.time()\n",
    "print(\"*******************************************************\")\n",
    "print(\"Total time cost for LPA:\", t_LPA_total_1 - t_LPA_total_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "logical-commerce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost of connected components: 149.9858365058899\n",
      "time cost to find sub_graph_list: 6.743970632553101\n",
      "Total time cost for LPA: 2990.0800545215607\n"
     ]
    }
   ],
   "source": [
    "print(\"time cost of connected components:\",t_con)\n",
    "\n",
    "print(\"time cost to find sub_graph_list:\", t_find_minmax)\n",
    "\n",
    "print(\"Total time cost for LPA:\", t_LPA_total_1 - t_LPA_total_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-antibody",
   "metadata": {},
   "source": [
    "## Compute ascending 2-manifolds (influence regions of minima), which are the connected components of Graph_VE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "coral-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n",
      "time cost of connected components: 88.56818056106567\n"
     ]
    }
   ],
   "source": [
    "t_con_0 = time.time()\n",
    "# result_con consists of two columns, vertex id, component\n",
    "result_con = graph_VE.connectedComponents()\n",
    "result_con.printSchema()\n",
    "\n",
    "t_con_1 = time.time()\n",
    "t_con = t_con_1 - t_con_0\n",
    "print(\"time cost of connected components:\",t_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-orchestra",
   "metadata": {},
   "source": [
    "# Methods to build Graph ET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-terrain",
   "metadata": {},
   "source": [
    "#### prepare nodes of Graph ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "coordinate-refund",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tri_order: integer (nullable = true)\n",
      " |-- tri: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all triangles are nodes of graph V2\n",
    "df_graph_V2_node = df_tri_order.select(col(\"tri_order\"), col(\"tri\"))\n",
    "df_graph_V2_node.printSchema()\n",
    "#df_graph_V2_node.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "sized-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tri_order: integer (nullable = true)\n",
      " |-- tri: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add a special triangle [-1, -1, -1] as node -1; this is useful to deal with boundary elements\n",
    "# e.g., for a boundary edge, it has only one coboundary triangle; to guarantee a consistent encoding, each edge has two coboundary triangels, the other coboundary triangle for a boundary edge is [-1,-1,-1]\n",
    "# columns_special = ['tri', 'V2_node']\n",
    "value_special = [(-1, [-1, -1, -1])]\n",
    "schema_special = StructType([\n",
    "    StructField('tri_order', IntegerType(),True),\n",
    "    StructField(\"tri\", ArrayType(IntegerType()),True)    \n",
    "])\n",
    "df_special_V2_node = spark.createDataFrame(value_special, schema_special)\n",
    "df_special_V2_node.printSchema()\n",
    "#df_special_V2_node.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-litigation",
   "metadata": {},
   "source": [
    "##### nodes of Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "amber-cherry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_graph_V2_node = df_graph_V2_node.union(df_special_V2_node)\n",
    "df_graph_V2_node_final = df_graph_V2_node.select(col(\"tri_order\").alias(\"id\"))\n",
    "df_graph_V2_node_final.printSchema()\n",
    "#df_graph_V2_node_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "weird-popularity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost to save Graph_ET nodes in Parquet: 6.628036499023438e-05\n"
     ]
    }
   ],
   "source": [
    "# df_graph_V2_node = df_graph_V2_node.repartition(20) date_name\n",
    "t_save_node_0 = time.time()\n",
    "\n",
    "file_df_graph_V2_node = 'SigSpatial_data' + '/' + tin_filename + '_ET_node' + '_' + '03132024' + '.parquet'\n",
    "# df_graph_V2_node_final.write.parquet(file_df_graph_V2_node) \n",
    "\n",
    "t_save_node_1 = time.time()\n",
    "print(\"Time cost to save Graph_ET nodes in Parquet:\", t_save_node_1 - t_save_node_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-transaction",
   "metadata": {},
   "source": [
    "#### prepare arcs of Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "appointed-enclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      " |-- ET_pair: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ET_pair_E: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- ET_pair_T: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method: firstly extract df_Forman_ET_pair from df_Forman\n",
    "df_Forman_ET_pair = df_Forman.select(col(\"Ver\"), col(\"VT_filtra\"), col(\"Forman.ET_pair\").alias(\"ET_pair\"))\n",
    "df_Forman_ET_pair.printSchema()\n",
    "# df_Forman_ET_pair.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "operational-thompson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- ET_pair_inTri: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ET_pair_co_t: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- ET_pair_t: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve co-boundary triangles (ET relation)\n",
    "def get_co_tri(VT, ET_pair):\n",
    "    if len(ET_pair) > 0:\n",
    "        co_tri_pair = []\n",
    "        num_ET_pair = len(ET_pair) # obtain the number of ET_pairs\n",
    "        def Co_tri(e,t,vt):\n",
    "            # e is sorted\n",
    "            for t_temp in vt:\n",
    "                # e0 = sorted([vt[0], vt[1]], reverse=True) # sort the edge in descending order\n",
    "                e0 = [t_temp[0], t_temp[1]] # e0, e1, and e2 are in descending order by default\n",
    "                e1 = [t_temp[0], t_temp[2]]\n",
    "                e2 = [t_temp[1], t_temp[2]]\n",
    "                t = [int(t[0]), int(t[1]), int(t[2])]\n",
    "                t_temp = [int(t_temp[0]), int(t_temp[1]), int(t_temp[2])]\n",
    "                # if e==e0 or e==e1 or e==e2 and t != t_temp:\n",
    "                if e==e0 or e==e1 or e==e2:\n",
    "                    if t != t_temp:\n",
    "                        return t_temp # t_temp is a coboundary triangle of t in vt\n",
    "                \n",
    "        for i in range(num_ET_pair):\n",
    "            ET_pair_e = ET_pair[i]['ET_pair_E'] # the i-th ET_pair\n",
    "            ET_pair_t = ET_pair[i]['ET_pair_T']\n",
    "            ET_pair_co_t = Co_tri(ET_pair_e, ET_pair_t, VT)\n",
    "            if ET_pair_co_t is None: # len(ET_pair_co_t) is empty, boundary triangles\n",
    "                ET_pair_co_t = [-1, -1, -1]\n",
    "            co_tri_pair.append([ET_pair_co_t, ET_pair_t])\n",
    "            \n",
    "        return co_tri_pair\n",
    "    \n",
    "# the whole StructType\n",
    "get_co_tri_schema = StructType([\n",
    "    StructField(\"ET_pair_co_t\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"ET_pair_t\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_co_tri_udf = udf(get_co_tri, ArrayType(get_co_tri_schema))\n",
    "\n",
    "df_Forman_V2_edge = df_Forman_ET_pair.withColumn(\"ET_pair_inTri\", get_co_tri_udf(df_Forman_ET_pair.VT_filtra, df_Forman_ET_pair.ET_pair))\n",
    "df_Forman_V2_edge = df_Forman_V2_edge.select(col(\"Ver\"), col(\"ET_pair_inTri\"))\n",
    "df_Forman_V2_edge.printSchema()\n",
    "# df_Forman_V2_edge.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-impossible",
   "metadata": {},
   "source": [
    "##### construct Graph_ET reversely, set src nodes as dst nodes, and dst nodes as src nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "baking-remedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arc_dst: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- arc_src: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode df_Forman_V2_edge\n",
    "df_Forman_V2_edge_split = df_Forman_V2_edge.select(explode(df_Forman_V2_edge.ET_pair_inTri).alias(\"ET_pair_src_dst\"))\n",
    "\n",
    "df_Forman_V2_edge_split = df_Forman_V2_edge_split.select(col(\"ET_pair_src_dst.ET_pair_co_t\").alias(\"arc_dst\"), col(\"ET_pair_src_dst.ET_pair_t\").alias(\"arc_src\")).na.drop()\n",
    "df_Forman_V2_edge_split.printSchema()\n",
    "# df_Forman_V2_edge_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aquatic-colonial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arc_dst: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- src: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join two DataFrames by df_Forman_V2_edge_split.arc_src == df_Forman_V2_edge_split.arc_des\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
    "df_Forman_V2_edge_split_src = df_Forman_V2_edge_split.join(df_graph_V2_node, df_Forman_V2_edge_split.arc_src == df_graph_V2_node.tri, \"left\")\n",
    "df_Forman_V2_edge_split_src = df_Forman_V2_edge_split_src.select(col(\"arc_dst\"), col(\"tri_order\").alias(\"src\"))\n",
    "df_Forman_V2_edge_split_src.printSchema()\n",
    "#df_Forman_V2_edge_split_src.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pending-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join two DataFrames by df_Forman_V2_edge_split.arc_src == df_Forman_V2_edge_split.arc_des\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
    "df_Forman_V2_edge_split_dst = df_Forman_V2_edge_split_src.join(df_graph_V2_node, df_Forman_V2_edge_split_src.arc_dst == df_graph_V2_node.tri, \"left\")\n",
    "df_Forman_V2_edge_split_dst = df_Forman_V2_edge_split_dst.select(col(\"src\"), col(\"tri_order\").alias(\"dst\"))\n",
    "# df_Forman_V2_edge_split_dst.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-dividend",
   "metadata": {},
   "source": [
    "##### arcs of Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "relative-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_graph_V2_edge_final = df_Forman_V2_edge_split_dst\n",
    "df_graph_V2_edge_final.printSchema()\n",
    "# df_graph_V2_edge_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aboriginal-authentication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost to save Graph_ET arcs in Parquet: 6.890296936035156e-05\n"
     ]
    }
   ],
   "source": [
    "# df_graph_V2_arc = df_graph_V2_arc.repartition(20)\n",
    "# print(\"number of partitions for df_graph_V2_arc:\", df_graph_V2_arc.rdd.getNumPartitions())\n",
    "\n",
    "# save Graph_VE arcs to Parquet\n",
    "t_save_arc_0 = time.time()\n",
    "file_df_graph_V2_arc = 'SigSpatial_data' + '/' + tin_filename + '_ET_arc' + '_' + '03132024' + '.parquet'\n",
    "# df_graph_V2_edge_final.write.parquet(file_df_graph_V2_arc) \n",
    "\n",
    "t_save_arc_1 = time.time()\n",
    "print(\"Time cost to save Graph_ET arcs in Parquet:\", t_save_arc_1 - t_save_arc_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-friendship",
   "metadata": {},
   "source": [
    "##### OR get nodes and arcs from previously saved results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forbidden-electric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions for df_graph_V2_node_final: 121\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      "\n",
      "Number of partitions for df_graph_V2_edge_final: 200\n",
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read nodes of V2\n",
    "schema_df_graph_V2_node_final = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True)\n",
    "  ])\n",
    "file_df_graph_V2_node_final_hdfs = 'SigSpatial_data' + '/' + tin_filename + '_graph_V2_node_final.parquet'\n",
    "df_graph_V2_node_final = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_graph_V2_node_final)\\\n",
    "      .load(file_df_graph_V2_node_final_hdfs)\n",
    "print(\"Number of partitions for df_graph_V2_node_final:\", df_graph_V2_node_final.rdd.getNumPartitions())\n",
    "df_graph_V2_node_final.printSchema()\n",
    "\n",
    "# read edges of V2\n",
    "schema_df_graph_V2_edge_final = StructType([ \\\n",
    "    StructField(\"src\",IntegerType(),True), \\\n",
    "    StructField(\"dst\",IntegerType(),True)\n",
    "  ])\n",
    "file_df_graph_V2_edge_final_hdfs = 'SigSpatial_data' + '/' + tin_filename + '_graph_V2_edge_final.parquet'\n",
    "df_graph_V2_edge_final = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_graph_V2_edge_final)\\\n",
    "      .load(file_df_graph_V2_edge_final_hdfs)\n",
    "print(\"Number of partitions for df_graph_V2_edge_final:\", df_graph_V2_edge_final.rdd.getNumPartitions())\n",
    "df_graph_V2_edge_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-underwear",
   "metadata": {},
   "source": [
    "## Construct a Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "established-temple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of graph_V2: <class 'graphframes.graphframe.GraphFrame'>\n"
     ]
    }
   ],
   "source": [
    "graph_ET = GraphFrame(df_graph_V2_node_final, df_graph_V2_edge_final)\n",
    "\n",
    "print(\"type of graph_V2:\", type(graph_ET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "reverse-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a checkpoint directory to improve performance\n",
    "# Checkpointing regularly helps recover from failures, clean shuffle files, shorten the\n",
    "# lineage of the computation graph, and reduce the complexity of plan optimization.\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-cancer",
   "metadata": {},
   "source": [
    "# Compute descending 2-manifolds (influence regions of maxima), which are the connected components of Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "reliable-giant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n",
      "time cost of connected components: 227.86275434494019\n"
     ]
    }
   ],
   "source": [
    "t_con_0 = time.time()\n",
    "# result_con consists of two columns, vertex id, component\n",
    "result_con = graph_ET.connectedComponents()\n",
    "result_con.printSchema()\n",
    "\n",
    "t_con_1 = time.time()\n",
    "t_con = t_con_1 - t_con_0\n",
    "print(\"time cost of connected components:\",t_con)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-landing",
   "metadata": {},
   "source": [
    "##### 1) left join result_con with saddles tri to identify the component that each saddles tri belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "laden-addition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ver: integer (nullable = true)\n",
      " |-- Saddle_edge: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- VT_filtra: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join two DataFrames to obatin VE relation\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
    "df_crit_E_VT = df_crit_E.join(df_VT, df_crit_E.Ver == df_VT.Ver, \"left\")\n",
    "df_crit_E_VT = df_crit_E_VT.select(df_crit_E.Ver, \"Saddle_edge\", \"VT_filtra\")\n",
    "df_crit_E_VT.printSchema()\n",
    "# df_crit_E_VT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "systematic-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- saddle_edge_Tri: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve co-boundary triangles (ET relation)\n",
    "def get_saddle_edge_tri(Saddle_edge, VT):\n",
    "    if len(VT) > 0:\n",
    "        e = Saddle_edge\n",
    "        tri_12 = []\n",
    "        num_VT = len(VT) # obtain the number of ET_pairs\n",
    "        for t_temp in VT:\n",
    "            e0 = [t_temp[0], t_temp[1]] # e0, e1, and e2 are in descending order by default\n",
    "            e1 = [t_temp[0], t_temp[2]]\n",
    "            e2 = [t_temp[1], t_temp[2]]\n",
    "            if e==e0 or e==e1 or e==e2:\n",
    "                tri_12.append(t_temp)\n",
    "            # if len(tri_12) == 2: # Maybe this triangle is at the boundary, so it has only one corresponding triangle in VT\n",
    "                # break\n",
    "        '''\n",
    "        if len(tri_12) == 1:\n",
    "            t_special = [-1,-1,-1]\n",
    "            tri_12.append(t_special)\n",
    "        '''\n",
    "            \n",
    "        return tri_12\n",
    "    \n",
    "# the whole StructType\n",
    "get_saddle_edge_tri_schema = StructType([\n",
    "    StructField(\"Saddle_edge_t1\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"Saddle_edge_t2\", ArrayType(IntegerType()), True)\n",
    "])\n",
    "\n",
    "# convert a function to an udf and determine the return type\n",
    "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "get_saddle_edge_tri_udf = udf(get_saddle_edge_tri, ArrayType(ArrayType(IntegerType())))\n",
    "\n",
    "df_crit_E_tri = df_crit_E_VT.withColumn(\"saddle_edge_Tri\", get_saddle_edge_tri_udf(df_crit_E_VT.Saddle_edge, df_crit_E_VT.VT_filtra))\n",
    "df_crit_E_tri = df_crit_E_tri.select(col(\"saddle_edge_Tri\"))\n",
    "df_crit_E_tri.printSchema()\n",
    "# df_crit_E_tri.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "organized-august",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- saddle_edge_Tri_split: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain the saddle tri\n",
    "df_crit_E2tri = df_crit_E_tri.select(explode(\"saddle_edge_Tri\").alias(\"saddle_edge_Tri_split\"))\n",
    "# some nodes may belong to multiple saddles, we need to remove them\n",
    "df_crit_E2tri = df_crit_E2tri.distinct()\n",
    "df_crit_E2tri.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "accompanied-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- saddle_edge_Tri_split_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join df_crit_E2tri with df_graph_V2_node to obtain corresponding id of tri\n",
    "df_crit_E2tri = df_crit_E2tri.join(df_graph_V2_node, df_crit_E2tri.saddle_edge_Tri_split == df_graph_V2_node.tri, \"left\")\n",
    "df_crit_E2tri = df_crit_E2tri.select(col('tri_order').alias('saddle_edge_Tri_split_id'))\n",
    "df_crit_E2tri.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "illegal-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- saddle_edge_Tri_split_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR get df_crit_E2pts from previously saved results\n",
    "\n",
    "schema_df_crit_E2tri = StructType([ \\\n",
    "    StructField(\"saddle_edge_Tri_split_id\",IntegerType(),True)\n",
    "  ])\n",
    "\n",
    "file_df_crit_E2tri_hdfs = 'hdfs_data' + '/' + tin_filename + '_crit_E2tri.parquet'\n",
    "#file_df_crit_E2tri_hdfs = 'SigSpatial_data' + '/' + tin_filename + '_crit_E2tri.parquet'\n",
    "\n",
    "df_crit_E2tri = spark.read.format(\"parquet\") \\\n",
    "      .option(\"header\", False) \\\n",
    "      .schema(schema_df_crit_E2tri)\\\n",
    "      .load(file_df_crit_E2tri_hdfs)\n",
    "\n",
    "df_crit_E2tri_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "duplicate-botswana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of partitions for result_con_saddle: 200\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      " |-- att: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_con_ET = result_con\n",
    "# identify the saddle tri that each component have\n",
    "result_con_saddle_ET = result_con_ET.join(df_crit_E2tri,result_con_ET.id==df_crit_E2tri.saddle_edge_Tri_split_id, \"inner\")\n",
    "result_con_saddle_ET = result_con_saddle_ET.withColumn(\"att\", when(result_con_saddle_ET.saddle_edge_Tri_split_id.isNull() == False, result_con_saddle_ET.saddle_edge_Tri_split_id).otherwise(-1)).drop('saddle_edge_Tri_split_id')\n",
    "\n",
    "print(\"number of partitions for result_con_saddle:\", result_con_saddle_ET.rdd.getNumPartitions())\n",
    "result_con_saddle_ET.printSchema()\n",
    "# result_con_saddle.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atomic-standing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- component: long (nullable = true)\n",
      " |-- multi_SdlTris: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# obtain all saddle tris for each connected component\n",
    "unique_SdlTris_per_con_ET = result_con_saddle_ET.groupBy('component').agg(collect_list('id').alias('multi_SdlTris'))\n",
    "unique_SdlTris_per_con_ET.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-giant",
   "metadata": {},
   "source": [
    "##### 2) left join df_graph_V2_edge_final with result_con to identify the connected component that each arc belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rotary-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- src: integer (nullable = true)\n",
      " |-- dst: integer (nullable = true)\n",
      " |-- component: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join df_graph_V1_arc with result_con to identify which component that each arc belongs to\n",
    "df_G_V2_arc_component = df_graph_V2_edge_final.join(result_con_ET,df_graph_V2_edge_final.src==result_con_ET.id, \"left\").drop('id')\n",
    "df_G_V2_arc_component.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-landscape",
   "metadata": {},
   "source": [
    "##### df_graph_V2_arc_component is the arcs of Graph_ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adapted-damages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost to find sub_graph_list: 6.009696960449219\n"
     ]
    }
   ],
   "source": [
    "Num_subgraphs = 20\n",
    "t_find_minmax_0 = time.time()\n",
    "\n",
    "# Find the minimum and maximum values of \"component\" in result_con\n",
    "sub_graph_tmp_min = result_con.agg({\"component\": \"min\"}).collect()[0][0]\n",
    "sub_graph_tmp_max = result_con.agg({\"component\": \"max\"}).collect()[0][0]\n",
    "\n",
    "sub_graph_list = []\n",
    "for i in range(Num_subgraphs+1):\n",
    "    sub_graph_tmp = int(i * (sub_graph_tmp_max - sub_graph_tmp_min)/(Num_subgraphs) + sub_graph_tmp_min)\n",
    "    sub_graph_list.append(sub_graph_tmp)\n",
    "    \n",
    "'''\n",
    "\n",
    "# sort and collect result_con_unique\n",
    "result_con_unique = result_con.groupBy('component').count()\n",
    "# result_con_unique.printSchema()\n",
    "# result_con_unique.show()\n",
    "\n",
    "result_con_unique = result_con_unique.sort(col('component'))\n",
    "result_con_unique_col = result_con_unique.collect()\n",
    "\n",
    "sub_graph_list = []\n",
    "\n",
    "for i in range(Num_subgraphs):\n",
    "    tmp = int(i / (Num_subgraphs-1) * (len(result_con_unique_col) -1))\n",
    "    sub_graph_tmp = result_con_unique_col[tmp]['component']\n",
    "    sub_graph_list.append(sub_graph_tmp)\n",
    "'''    \n",
    "\n",
    "t_find_minmax_1 = time.time()\n",
    "t_find_minmax = t_find_minmax_1 - t_find_minmax_0\n",
    "print(\"time cost to find sub_graph_list:\", t_find_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "stylish-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame with a schema\n",
    "schema_special = StructType([\n",
    "    StructField('id', LongType(),True),\n",
    "    StructField(\"component\", LongType(),True),\n",
    "    StructField(\"att\", LongType(),True),   \n",
    "    StructField(\"label\", LongType(),True)   \n",
    "])\n",
    "\n",
    "df_empty = spark.createDataFrame([], schema_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "married-continent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost for LPA to subgraph -1: 112.242347\n",
      "Time cost for LPA to subgraph 4898863: 87.272774\n",
      "Time cost for LPA to subgraph 9797727: 88.380053\n",
      "Time cost for LPA to subgraph 14696591: 84.740761\n",
      "Time cost for LPA to subgraph 19595455: 84.608591\n",
      "Time cost for LPA to subgraph 24494320: 79.805896\n",
      "Time cost for LPA to subgraph 29393184: 79.376734\n",
      "Time cost for LPA to subgraph 34292048: 80.889919\n",
      "Time cost for LPA to subgraph 39190912: 74.401150\n",
      "Time cost for LPA to subgraph 44089776: 74.403847\n",
      "Time cost for LPA to subgraph 48988641: 76.381688\n",
      "Time cost for LPA to subgraph 53887505: 75.135764\n",
      "Time cost for LPA to subgraph 58786369: 84.505844\n",
      "Time cost for LPA to subgraph 63685233: 74.865218\n",
      "Time cost for LPA to subgraph 68584097: 77.825959\n",
      "Time cost for LPA to subgraph 73482962: 76.659230\n",
      "Time cost for LPA to subgraph 78381826: 82.205308\n",
      "Time cost for LPA to subgraph 83280690: 77.444861\n",
      "Time cost for LPA to subgraph 88179554: 77.599617\n",
      "Time cost for LPA to subgraph 93078418: 76.883111\n",
      "*******************************************************\n",
      "Total time cost for LPA: 1625.8993074893951\n"
     ]
    }
   ],
   "source": [
    "# construct subgraphs and union the LPA results\n",
    "t_LPA_total_0 = time.time()\n",
    "\n",
    "sub_time = [] # to store time cost for each subgraph\n",
    "for i in range(len(sub_graph_list) - 1):\n",
    "    t_LPA_sub_0 = time.time()\n",
    "    sub_graph_tmp_0 = sub_graph_list[i]\n",
    "    sub_graph_tmp_1 = sub_graph_list[i+1]\n",
    "    \n",
    "    if i == len(sub_graph_list) - 1: # the last subgraphs should contain sub_graph_tmp_max\n",
    "        sub_graph_node = result_con_saddle_ET.filter((result_con_saddle_ET.component >= sub_graph_tmp_0) & (result_con_saddle_ET.component <= sub_graph_tmp_1))\n",
    "        sub_graph_arc = df_G_V2_arc_component.filter((df_G_V2_arc_component.component >= sub_graph_tmp_0) & (df_G_V2_arc_component.component <= sub_graph_tmp_1)).drop('id')\n",
    "    else:\n",
    "        sub_graph_node = result_con_saddle_ET.filter((result_con_saddle_ET.component >= sub_graph_tmp_0) & (result_con_saddle_ET.component < sub_graph_tmp_1))\n",
    "        # sub_graph_node.printSchema() # 3 columns: id, component, att\n",
    "        sub_graph_arc = df_G_V2_arc_component.filter((df_G_V2_arc_component.component >= sub_graph_tmp_0) & (df_G_V2_arc_component.component < sub_graph_tmp_1)).drop('id')\n",
    "        # sub_graph_arc.printSchema() # 3 columns: src, dst, component\n",
    "    \n",
    "    sub_graph = GraphFrame(sub_graph_node, sub_graph_arc)\n",
    "    \n",
    "    sub_graph_LPA = sub_graph.pregel \\\n",
    "         .setMaxIter(10) \\\n",
    "         .withVertexColumn(\"label\", col(\"att\"), \\\n",
    "             Pregel.msg()) \\\n",
    "         .sendMsgToDst(when(Pregel.dst(\"label\")>0, Pregel.dst(\"label\")).otherwise(Pregel.src(\"label\"))) \\\n",
    "         .aggMsgs(F.max(Pregel.msg())) \\\n",
    "         .run()\n",
    "    t_LPA_sub_1 = time.time()\n",
    "    df_empty = df_empty.union(sub_graph_LPA)\n",
    "    print(\"Time cost for LPA to subgraph %ld: %f\" % (sub_graph_tmp_0, t_LPA_sub_1 - t_LPA_sub_0))\n",
    "    sub_time.append(t_LPA_sub_1 - t_LPA_sub_0)\n",
    "    \n",
    "t_LPA_total_1 = time.time()\n",
    "print(\"*******************************************************\")\n",
    "print(\"Total time cost for LPA:\", t_LPA_total_1 - t_LPA_total_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "macro-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost of connected components: 227.86275434494019\n",
      "time cost to find sub_graph_list: 6.009696960449219\n",
      "Total time cost for LPA: 1625.8993074893951\n"
     ]
    }
   ],
   "source": [
    "print(\"time cost of connected components:\",t_con)\n",
    "\n",
    "print(\"time cost to find sub_graph_list:\", t_find_minmax)\n",
    "\n",
    "print(\"Total time cost for LPA:\", t_LPA_total_1 - t_LPA_total_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-discretion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
